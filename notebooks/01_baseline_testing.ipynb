{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Agent Testing\n",
    "\n",
    "This notebook tests the two baseline agents:\n",
    "1. **Shortest Path (A*)**: Deterministic optimal pathfinding\n",
    "2. **Random Agent**: Uniformly random action selection\n",
    "\n",
    "These baselines establish performance bounds for RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core.environment import SnakeEnv\n",
    "from scripts.baselines.shortest_path import ShortestPathAgent\n",
    "from scripts.baselines.random_agent import RandomAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "GRID_SIZE = 10\n",
    "N_EPISODES = 100\n",
    "MAX_STEPS = 1000\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, n_episodes=100, max_steps=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Test agent over multiple episodes\n",
    "    \n",
    "    Returns:\n",
    "        scores, rewards, lengths\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset(seed=seed + episode)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(env)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        scores.append(info['score'])\n",
    "        rewards.append(total_reward)\n",
    "        lengths.append(step + 1)\n",
    "    \n",
    "    return np.array(scores), np.array(rewards), np.array(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Shortest Path Agent (Absolute Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing Shortest Path Agent with Absolute Actions...\")\n\nenv_astar_abs = SnakeEnv(\n    grid_size=GRID_SIZE,\n    action_space_type='absolute',\n    state_representation='feature',\n    max_steps=MAX_STEPS\n)\n\nagent_astar_abs = ShortestPathAgent(action_space_type='absolute')\n\nscores_astar_abs, rewards_astar_abs, lengths_astar_abs = test_agent(\n    env_astar_abs, agent_astar_abs, N_EPISODES, MAX_STEPS, SEED\n)\n\nprint(f\"\\nA* (Absolute) Results:\")\nprint(f\"  Avg Score: {scores_astar_abs.mean():.2f} +/- {scores_astar_abs.std():.2f}\")\nprint(f\"  Max Score: {scores_astar_abs.max()}\")\nprint(f\"  Avg Reward: {rewards_astar_abs.mean():.2f} +/- {rewards_astar_abs.std():.2f}\")\nprint(f\"  Avg Length: {lengths_astar_abs.mean():.2f} +/- {lengths_astar_abs.std():.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Shortest Path Agent (Relative Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing Shortest Path Agent with Relative Actions...\")\n\nenv_astar_rel = SnakeEnv(\n    grid_size=GRID_SIZE,\n    action_space_type='relative',\n    state_representation='feature',\n    max_steps=MAX_STEPS\n)\n\nagent_astar_rel = ShortestPathAgent(action_space_type='relative')\n\nscores_astar_rel, rewards_astar_rel, lengths_astar_rel = test_agent(\n    env_astar_rel, agent_astar_rel, N_EPISODES, MAX_STEPS, SEED\n)\n\nprint(f\"\\nA* (Relative) Results:\")\nprint(f\"  Avg Score: {scores_astar_rel.mean():.2f} +/- {scores_astar_rel.std():.2f}\")\nprint(f\"  Max Score: {scores_astar_rel.max()}\")\nprint(f\"  Avg Reward: {rewards_astar_rel.mean():.2f} +/- {rewards_astar_rel.std():.2f}\")\nprint(f\"  Avg Length: {lengths_astar_rel.mean():.2f} +/- {lengths_astar_rel.std():.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Random Agent (Absolute Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing Random Agent with Absolute Actions...\")\n\nenv_rand_abs = SnakeEnv(\n    grid_size=GRID_SIZE,\n    action_space_type='absolute',\n    state_representation='feature',\n    max_steps=MAX_STEPS\n)\n\nagent_rand_abs = RandomAgent(action_space_type='absolute', seed=SEED)\n\nscores_rand_abs, rewards_rand_abs, lengths_rand_abs = test_agent(\n    env_rand_abs, agent_rand_abs, N_EPISODES, MAX_STEPS, SEED\n)\n\nprint(f\"\\nRandom (Absolute) Results:\")\nprint(f\"  Avg Score: {scores_rand_abs.mean():.2f} +/- {scores_rand_abs.std():.2f}\")\nprint(f\"  Max Score: {scores_rand_abs.max()}\")\nprint(f\"  Avg Reward: {rewards_rand_abs.mean():.2f} +/- {rewards_rand_abs.std():.2f}\")\nprint(f\"  Avg Length: {lengths_rand_abs.mean():.2f} +/- {lengths_rand_abs.std():.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Random Agent (Relative Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing Random Agent with Relative Actions...\")\n\nenv_rand_rel = SnakeEnv(\n    grid_size=GRID_SIZE,\n    action_space_type='relative',\n    state_representation='feature',\n    max_steps=MAX_STEPS\n)\n\nagent_rand_rel = RandomAgent(action_space_type='relative', seed=SEED)\n\nscores_rand_rel, rewards_rand_rel, lengths_rand_rel = test_agent(\n    env_rand_rel, agent_rand_rel, N_EPISODES, MAX_STEPS, SEED\n)\n\nprint(f\"\\nRandom (Relative) Results:\")\nprint(f\"  Avg Score: {scores_rand_rel.mean():.2f} +/- {scores_rand_rel.std():.2f}\")\nprint(f\"  Max Score: {scores_rand_rel.max()}\")\nprint(f\"  Avg Reward: {rewards_rand_rel.mean():.2f} +/- {rewards_rand_rel.std():.2f}\")\nprint(f\"  Avg Length: {lengths_rand_rel.mean():.2f} +/- {lengths_rand_rel.std():.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Score comparison\n",
    "agents = ['A* (Abs)', 'A* (Rel)', 'Rand (Abs)', 'Rand (Rel)']\n",
    "score_means = [\n",
    "    scores_astar_abs.mean(),\n",
    "    scores_astar_rel.mean(),\n",
    "    scores_rand_abs.mean(),\n",
    "    scores_rand_rel.mean()\n",
    "]\n",
    "score_stds = [\n",
    "    scores_astar_abs.std(),\n",
    "    scores_astar_rel.std(),\n",
    "    scores_rand_abs.std(),\n",
    "    scores_rand_rel.std()\n",
    "]\n",
    "\n",
    "axes[0].bar(agents, score_means, yerr=score_stds, capsize=5)\n",
    "axes[0].set_ylabel('Average Score')\n",
    "axes[0].set_title('Score Comparison')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Reward comparison\n",
    "reward_means = [\n",
    "    rewards_astar_abs.mean(),\n",
    "    rewards_astar_rel.mean(),\n",
    "    rewards_rand_abs.mean(),\n",
    "    rewards_rand_rel.mean()\n",
    "]\n",
    "reward_stds = [\n",
    "    rewards_astar_abs.std(),\n",
    "    rewards_astar_rel.std(),\n",
    "    rewards_rand_abs.std(),\n",
    "    rewards_rand_rel.std()\n",
    "]\n",
    "\n",
    "axes[1].bar(agents, reward_means, yerr=reward_stds, capsize=5)\n",
    "axes[1].set_ylabel('Average Total Reward')\n",
    "axes[1].set_title('Reward Comparison')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Episode length comparison\n",
    "length_means = [\n",
    "    lengths_astar_abs.mean(),\n",
    "    lengths_astar_rel.mean(),\n",
    "    lengths_rand_abs.mean(),\n",
    "    lengths_rand_rel.mean()\n",
    "]\n",
    "length_stds = [\n",
    "    lengths_astar_abs.std(),\n",
    "    lengths_astar_rel.std(),\n",
    "    lengths_rand_abs.std(),\n",
    "    lengths_rand_rel.std()\n",
    "]\n",
    "\n",
    "axes[2].bar(agents, length_means, yerr=length_stds, capsize=5)\n",
    "axes[2].set_ylabel('Average Episode Length')\n",
    "axes[2].set_title('Episode Length Comparison')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/baseline_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBaseline comparison plot saved to results/figures/baseline_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "1. **A* Agent (Both Action Spaces)**:\n",
    "   - Should achieve high scores (10-30 depending on grid complexity)\n",
    "   - Deterministic optimal pathfinding\n",
    "   - Performance upper bound for RL agents\n",
    "\n",
    "2. **Random Agent (Both Action Spaces)**:\n",
    "   - Very low scores (0-2 typically)\n",
    "   - Short episode lengths\n",
    "   - Performance lower bound\n",
    "\n",
    "3. **Action Space Comparison**:\n",
    "   - Absolute and relative should perform similarly for A*\n",
    "   - Random may vary slightly between action spaces\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- RL agents should aim to exceed random agent performance quickly\n",
    "- Goal is to approach or match A* performance\n",
    "- These baselines will be used for comparison in all training notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}