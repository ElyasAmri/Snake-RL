{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Two-Snake MLP Training Notebook\n",
    "\n",
    "This notebook trains two PPO agents to compete in a two-snake environment.\n",
    "\n",
    "**Algorithm**: PPO Two-Snake with:\n",
    "- Vectorized environment (128 parallel games)\n",
    "- 35-dimensional feature vector per snake\n",
    "- Big snake: 256x256 network, Small snake: 128x128 network\n",
    "- Direct co-evolution (both agents learn simultaneously)\n",
    "- Rollout-based training with GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from core.environment_two_snake_vectorized import VectorizedTwoSnakeEnv\n",
    "from core.networks import PPO_Actor_MLP, PPO_Critic_MLP\n",
    "from core.utils import set_seed, get_device\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (papermill parameters)\n",
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Environment\n",
    "NUM_ENVS = 128\n",
    "GRID_SIZE = 20\n",
    "TARGET_FOOD = 10\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Network sizes\n",
    "BIG_HIDDEN_DIMS = (256, 256)  # Agent 1\n",
    "SMALL_HIDDEN_DIMS = (128, 128)  # Agent 2\n",
    "\n",
    "# PPO hyperparameters\n",
    "ACTOR_LR = 0.0003\n",
    "CRITIC_LR = 0.0003\n",
    "ROLLOUT_STEPS = 2048\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_PER_ROLLOUT = 4\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "VALUE_LOSS_COEF = 0.5\n",
    "ENTROPY_COEF = 0.1\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# Training\n",
    "TOTAL_STEPS = 10000  # Short for testing (full: 250000)\n",
    "LOG_INTERVAL = 100\n",
    "\n",
    "# Output\n",
    "SAVE_DIR = '../../results/weights/ppo_two_snake_mlp'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Environment & Model Setup\n",
    "set_seed(SEED)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create environment\n",
    "env = VectorizedTwoSnakeEnv(\n",
    "    num_envs=NUM_ENVS,\n",
    "    grid_size=GRID_SIZE,\n",
    "    target_food=TARGET_FOOD,\n",
    "    max_steps=MAX_STEPS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# State and action dimensions\n",
    "state_dim = 35  # 35-dimensional feature vector per snake\n",
    "action_dim = 3  # relative actions\n",
    "\n",
    "# Create Agent 1 (Big) networks\n",
    "actor1 = PPO_Actor_MLP(state_dim, action_dim, BIG_HIDDEN_DIMS).to(device)\n",
    "critic1 = PPO_Critic_MLP(state_dim, BIG_HIDDEN_DIMS).to(device)\n",
    "\n",
    "# Create Agent 2 (Small) networks  \n",
    "actor2 = PPO_Actor_MLP(state_dim, action_dim, SMALL_HIDDEN_DIMS).to(device)\n",
    "critic2 = PPO_Critic_MLP(state_dim, SMALL_HIDDEN_DIMS).to(device)\n",
    "\n",
    "# Optimizers\n",
    "actor1_optim = optim.Adam(actor1.parameters(), lr=ACTOR_LR)\n",
    "critic1_optim = optim.Adam(critic1.parameters(), lr=CRITIC_LR)\n",
    "actor2_optim = optim.Adam(actor2.parameters(), lr=ACTOR_LR)\n",
    "critic2_optim = optim.Adam(critic2.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "print(f\"Environment: {GRID_SIZE}x{GRID_SIZE} grid, {NUM_ENVS} parallel games\")\n",
    "print(f\"Agent 1 (Big): {BIG_HIDDEN_DIMS[0]}x{BIG_HIDDEN_DIMS[1]} hidden\")\n",
    "print(f\"Agent 2 (Small): {SMALL_HIDDEN_DIMS[0]}x{SMALL_HIDDEN_DIMS[1]} hidden\")\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PPO Helper Functions\n",
    "\n",
    "class PPOBuffer:\n",
    "    \"\"\"Rollout buffer for PPO\"\"\"\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "    \n",
    "    def add(self, state, action, reward, done, log_prob, value):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "    \n",
    "    def get(self):\n",
    "        states = torch.stack(self.states)\n",
    "        actions = torch.stack(self.actions)\n",
    "        rewards = torch.stack(self.rewards)\n",
    "        dones = torch.stack(self.dones)\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.stack(self.values)\n",
    "        return states, actions, rewards, dones, log_probs, values\n",
    "\n",
    "def select_action(actor, critic, state):\n",
    "    \"\"\"Select action using policy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = actor(state)\n",
    "        value = critic(state).squeeze(-1)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "    return action, log_prob, value\n",
    "\n",
    "def compute_gae(rewards, values, dones, next_value, gamma=GAMMA, gae_lambda=GAE_LAMBDA):\n",
    "    \"\"\"Compute GAE advantages\"\"\"\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            next_val = next_value\n",
    "        else:\n",
    "            next_val = values[t + 1]\n",
    "        \n",
    "        mask = 1.0 - dones[t].float()\n",
    "        delta = rewards[t] + gamma * next_val * mask - values[t]\n",
    "        gae = delta + gamma * gae_lambda * mask * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = torch.stack(advantages)\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "def ppo_update(actor, critic, actor_optim, critic_optim, states, actions, old_log_probs, advantages, returns):\n",
    "    \"\"\"PPO update for one agent\"\"\"\n",
    "    # Flatten\n",
    "    states_flat = states.view(-1, states.shape[-1])\n",
    "    actions_flat = actions.view(-1)\n",
    "    old_log_probs_flat = old_log_probs.view(-1)\n",
    "    advantages_flat = advantages.view(-1)\n",
    "    returns_flat = returns.view(-1)\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages_flat = (advantages_flat - advantages_flat.mean()) / (advantages_flat.std() + 1e-8)\n",
    "    \n",
    "    total_actor_loss = 0\n",
    "    total_critic_loss = 0\n",
    "    n_updates = 0\n",
    "    \n",
    "    for _ in range(EPOCHS_PER_ROLLOUT):\n",
    "        indices = torch.randperm(states_flat.size(0))\n",
    "        \n",
    "        for start in range(0, states_flat.size(0), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            batch_idx = indices[start:end]\n",
    "            \n",
    "            # Get batch\n",
    "            batch_states = states_flat[batch_idx]\n",
    "            batch_actions = actions_flat[batch_idx]\n",
    "            batch_old_log_probs = old_log_probs_flat[batch_idx]\n",
    "            batch_advantages = advantages_flat[batch_idx]\n",
    "            batch_returns = returns_flat[batch_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = actor(batch_states)\n",
    "            values = critic(batch_states).squeeze(-1)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            log_probs = dist.log_prob(batch_actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # PPO loss\n",
    "            ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "            surr1 = ratio * batch_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * batch_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEF * entropy\n",
    "            \n",
    "            critic_loss = F.mse_loss(values, batch_returns)\n",
    "            \n",
    "            # Update\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor.parameters(), MAX_GRAD_NORM)\n",
    "            actor_optim.step()\n",
    "            \n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), MAX_GRAD_NORM)\n",
    "            critic_optim.step()\n",
    "            \n",
    "            total_actor_loss += actor_loss.item()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "            n_updates += 1\n",
    "    \n",
    "    return total_actor_loss / n_updates, total_critic_loss / n_updates\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Training Loop\nprint(\"Starting PPO Two-Snake Training...\")\nprint(f\"Total steps: {TOTAL_STEPS:,}\")\nprint()\n\nstart_time = time.time()\ntotal_steps = 0\nround_count = 0\n\n# Tracking\nall_win_rates = []\nall_scores1 = []\nall_scores2 = []\nround_winners = []\n\n# Buffers for both agents\nbuffer1 = PPOBuffer(device)\nbuffer2 = PPOBuffer(device)\n\nstate1, state2 = env.reset(seed=SEED)\n\nwhile total_steps < TOTAL_STEPS:\n    buffer1.clear()\n    buffer2.clear()\n    \n    # Collect rollout\n    for _ in range(ROLLOUT_STEPS // NUM_ENVS):\n        action1, log_prob1, value1 = select_action(actor1, critic1, state1)\n        action2, log_prob2, value2 = select_action(actor2, critic2, state2)\n        \n        # VectorizedTwoSnakeEnv returns 6 values\n        next_state1, next_state2, reward1, reward2, dones, info = env.step(action1, action2)\n        \n        buffer1.add(state1, action1, reward1, dones, log_prob1, value1)\n        buffer2.add(state2, action2, reward2, dones, log_prob2, value2)\n        \n        # Track wins on episode ends\n        if dones.any():\n            num_done = len(info['done_envs'])\n            for i in range(num_done):\n                winner = info['winners'][i]\n                round_winners.append(int(winner))\n                all_scores1.append(info['food_counts1'][i])\n                all_scores2.append(info['food_counts2'][i])\n                round_count += 1\n        \n        state1, state2 = next_state1, next_state2\n        total_steps += NUM_ENVS\n    \n    # Compute advantages and update both agents\n    with torch.no_grad():\n        next_value1 = critic1(state1).squeeze(-1)\n        next_value2 = critic2(state2).squeeze(-1)\n    \n    states1, actions1, rewards1, dones1, log_probs1, values1 = buffer1.get()\n    states2, actions2, rewards2, dones2, log_probs2, values2 = buffer2.get()\n    \n    advantages1, returns1 = compute_gae(rewards1, values1, dones1, next_value1)\n    advantages2, returns2 = compute_gae(rewards2, values2, dones2, next_value2)\n    \n    ppo_update(actor1, critic1, actor1_optim, critic1_optim, states1, actions1, log_probs1, advantages1, returns1)\n    ppo_update(actor2, critic2, actor2_optim, critic2_optim, states2, actions2, log_probs2, advantages2, returns2)\n    \n    # Logging\n    if total_steps % (LOG_INTERVAL * NUM_ENVS) < NUM_ENVS:\n        win_rate = sum(1 for w in round_winners[-100:] if w == 1) / max(1, len(round_winners[-100:]))\n        all_win_rates.append(win_rate)\n        \n        elapsed = time.time() - start_time\n        sps = total_steps / elapsed if elapsed > 0 else 0\n        \n        recent_s1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n        recent_s2 = np.mean(all_scores2[-100:]) if all_scores2 else 0\n        \n        print(f\"Step {total_steps:,}/{TOTAL_STEPS:,} | \"\n              f\"Rounds: {round_count} | \"\n              f\"A1 Score: {recent_s1:.2f} | \"\n              f\"A2 Score: {recent_s2:.2f} | \"\n              f\"Win Rate: {win_rate:.2%} | \"\n              f\"SPS: {sps:.0f}\")\n\ntraining_time = time.time() - start_time\nprint(f\"\\nTraining complete!\")\nprint(f\"Total time: {training_time:.1f}s\")\nprint(f\"Total rounds: {round_count}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Save Weights\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save Agent 1\n",
    "agent1_path = save_dir / f\"big_256x256_step{total_steps}_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    'actor': actor1.state_dict(),\n",
    "    'critic': critic1.state_dict(),\n",
    "    'actor_optimizer': actor1_optim.state_dict(),\n",
    "    'critic_optimizer': critic1_optim.state_dict(),\n",
    "    'total_steps': total_steps\n",
    "}, agent1_path)\n",
    "print(f\"Agent 1 saved to: {agent1_path}\")\n",
    "\n",
    "# Save Agent 2\n",
    "agent2_path = save_dir / f\"small_128x128_step{total_steps}_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    'actor': actor2.state_dict(),\n",
    "    'critic': critic2.state_dict(),\n",
    "    'actor_optimizer': actor2_optim.state_dict(),\n",
    "    'critic_optimizer': critic2_optim.state_dict(),\n",
    "    'total_steps': total_steps\n",
    "}, agent2_path)\n",
    "print(f\"Agent 2 saved to: {agent2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "def smooth(data, window=50):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot 1: Agent Scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(all_scores1, alpha=0.3, label='Agent 1 (raw)', color='blue')\n",
    "ax1.plot(all_scores2, alpha=0.3, label='Agent 2 (raw)', color='red')\n",
    "if len(all_scores1) > 50:\n",
    "    ax1.plot(range(49, len(all_scores1)), smooth(all_scores1), label='Agent 1 (smooth)', color='blue')\n",
    "    ax1.plot(range(49, len(all_scores2)), smooth(all_scores2), label='Agent 2 (smooth)', color='red')\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Agent Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Win Rate\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(all_win_rates, color='green')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--')\n",
    "ax2.set_xlabel('Log Step')\n",
    "ax2.set_ylabel('Agent 1 Win Rate')\n",
    "ax2.set_title('Win Rate Over Time')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Win Distribution\n",
    "ax3 = axes[1, 0]\n",
    "a1_wins = sum(1 for w in round_winners if w == 1)\n",
    "a2_wins = sum(1 for w in round_winners if w == 2)\n",
    "draws = sum(1 for w in round_winners if w == 0)\n",
    "labels = ['Agent 1', 'Agent 2', 'Draw']\n",
    "sizes = [a1_wins, a2_wins, draws]\n",
    "colors = ['blue', 'red', 'gray']\n",
    "ax3.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "ax3.set_title('Win Distribution')\n",
    "\n",
    "# Plot 4: Score Distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(all_scores1, bins=20, alpha=0.5, label='Agent 1', color='blue')\n",
    "ax4.hist(all_scores2, bins=20, alpha=0.5, label='Agent 2', color='red')\n",
    "ax4.set_xlabel('Score')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Score Distribution')\n",
    "ax4.legend()\n",
    "\n",
    "plt.suptitle(f'PPO Two-Snake Training - {total_steps:,} Steps', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = project_root / 'results' / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig_path = fig_dir / f'ppo_two_snake_mlp_training_{timestamp}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Results Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Algorithm: PPO Two-Snake (Co-Evolution)\")\n",
    "print(f\"Total Steps: {total_steps:,}\")\n",
    "print(f\"Total Rounds: {round_count}\")\n",
    "print(f\"Training Time: {training_time:.1f}s\")\n",
    "print()\n",
    "print(\"Match Results:\")\n",
    "print(f\"  Agent 1 Wins: {a1_wins} ({a1_wins/round_count*100:.1f}%)\")\n",
    "print(f\"  Agent 2 Wins: {a2_wins} ({a2_wins/round_count*100:.1f}%)\")\n",
    "print(f\"  Draws: {draws} ({draws/round_count*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Final Performance (last 100 rounds):\")\n",
    "print(f\"  Agent 1 Avg Score: {np.mean(all_scores1[-100:]):.2f}\")\n",
    "print(f\"  Agent 2 Avg Score: {np.mean(all_scores2[-100:]):.2f}\")\n",
    "print(f\"  Agent 1 Win Rate: {sum(1 for w in round_winners[-100:] if w == 1)/100:.2%}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}