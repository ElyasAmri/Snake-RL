{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# DQN Two-Snake Curriculum Training Notebook\n",
    "\n",
    "This notebook trains DQN agents using curriculum learning for two-snake competitive play.\n",
    "\n",
    "**Algorithm**: DQN with 5-stage curriculum:\n",
    "1. Stage 0: vs StaticAgent (learn basic movement, 70% win threshold)\n",
    "2. Stage 1: vs RandomAgent (handle unpredictability, 60% win threshold)\n",
    "3. Stage 2: vs GreedyFoodAgent (compete for food, 55% win threshold)\n",
    "4. Stage 3: vs Frozen small network (50% win threshold)\n",
    "5. Stage 4: Co-evolution (both learning, no threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from core.environment_two_snake_vectorized import VectorizedTwoSnakeEnv\n",
    "from core.networks import DQN_MLP\n",
    "from core.utils import ReplayBuffer, EpsilonScheduler, set_seed, get_device\n",
    "from scripts.baselines.scripted_opponents import get_scripted_agent\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "tags": ["parameters"]
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (papermill parameters)\n",
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Environment\n",
    "GRID_SIZE = 20\n",
    "NUM_ENVS = 128\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Network sizes\n",
    "BIG_HIDDEN_DIMS = (256, 256)  # Agent 1\n",
    "SMALL_HIDDEN_DIMS = (128, 128)  # Agent 2\n",
    "\n",
    "# DQN hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "TRAIN_STEPS_RATIO = 0.125  # Train every 8th step\n",
    "\n",
    "# Epsilon parameters\n",
    "EPSILON_START = 0.8\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "# Curriculum Stage Settings (min_steps for each)\n",
    "STAGE0_MIN_STEPS = 5000  # Static opponent\n",
    "STAGE1_MIN_STEPS = 7500  # Random opponent\n",
    "STAGE2_MIN_STEPS = 10000  # Greedy opponent\n",
    "STAGE3_MIN_STEPS = 10000  # Frozen policy\n",
    "STAGE4_MIN_STEPS = 20000  # Co-evolution\n",
    "\n",
    "# Curriculum thresholds\n",
    "STAGE0_WIN_THRESHOLD = 0.70\n",
    "STAGE1_WIN_THRESHOLD = 0.60\n",
    "STAGE2_WIN_THRESHOLD = 0.55\n",
    "STAGE3_WIN_THRESHOLD = 0.50\n",
    "\n",
    "# Target food per stage (progressive difficulty)\n",
    "STAGE0_TARGET_FOOD = 10\n",
    "STAGE1_TARGET_FOOD = 10\n",
    "STAGE2_TARGET_FOOD = 4\n",
    "STAGE3_TARGET_FOOD = 6\n",
    "STAGE4_TARGET_FOOD = 8\n",
    "\n",
    "# Output\n",
    "SAVE_DIR = '../../results/weights/dqn_two_snake_curriculum'\n",
    "SEED = 42\n",
    "LOG_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Environment & Agent Setup\n",
    "set_seed(SEED)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create environment\n",
    "env = VectorizedTwoSnakeEnv(\n",
    "    num_envs=NUM_ENVS,\n",
    "    grid_size=GRID_SIZE,\n",
    "    max_steps=MAX_STEPS,\n",
    "    target_food=STAGE0_TARGET_FOOD,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get input dimensions from environment\n",
    "obs1, obs2 = env.reset()\n",
    "input_dim = obs1.shape[1]  # Should be 35 for competitive features\n",
    "output_dim = 3  # relative actions\n",
    "\n",
    "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")\n",
    "\n",
    "# Create Agent 1 (Big) networks\n",
    "policy_net1 = DQN_MLP(input_dim, output_dim, BIG_HIDDEN_DIMS).to(device)\n",
    "target_net1 = DQN_MLP(input_dim, output_dim, BIG_HIDDEN_DIMS).to(device)\n",
    "target_net1.load_state_dict(policy_net1.state_dict())\n",
    "target_net1.eval()\n",
    "optimizer1 = optim.Adam(policy_net1.parameters(), lr=LEARNING_RATE)\n",
    "buffer1 = ReplayBuffer(capacity=BUFFER_SIZE, seed=SEED)\n",
    "\n",
    "# Create Agent 2 (Small) networks\n",
    "policy_net2 = DQN_MLP(input_dim, output_dim, SMALL_HIDDEN_DIMS).to(device)\n",
    "target_net2 = DQN_MLP(input_dim, output_dim, SMALL_HIDDEN_DIMS).to(device)\n",
    "target_net2.load_state_dict(policy_net2.state_dict())\n",
    "target_net2.eval()\n",
    "optimizer2 = optim.Adam(policy_net2.parameters(), lr=LEARNING_RATE)\n",
    "buffer2 = ReplayBuffer(capacity=BUFFER_SIZE, seed=SEED)\n",
    "\n",
    "# Load scripted opponents\n",
    "scripted_agents = {}\n",
    "for agent_type in ['static', 'random', 'greedy']:\n",
    "    try:\n",
    "        scripted_agents[agent_type] = get_scripted_agent(agent_type, device=device)\n",
    "        print(f\"Loaded {agent_type} opponent\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load {agent_type} agent: {e}\")\n",
    "\n",
    "print(f\"\\nEnvironment: {GRID_SIZE}x{GRID_SIZE} grid, {NUM_ENVS} parallel envs\")\n",
    "print(f\"Agent 1 (Big): {BIG_HIDDEN_DIMS}\")\n",
    "print(f\"Agent 2 (Small): {SMALL_HIDDEN_DIMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: DQN Helper Functions\n",
    "\n",
    "def select_action(policy_net, states, epsilon):\n",
    "    \"\"\"Select actions using epsilon-greedy policy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(states)\n",
    "        greedy_actions = q_values.argmax(dim=1)\n",
    "    \n",
    "    num_envs = states.shape[0]\n",
    "    random_mask = torch.rand(num_envs, device=device) < epsilon\n",
    "    random_actions = torch.randint(0, 3, (num_envs,), device=device)\n",
    "    actions = torch.where(random_mask, random_actions, greedy_actions)\n",
    "    return actions\n",
    "\n",
    "def select_greedy_action(policy_net, states):\n",
    "    \"\"\"Select greedy actions (for frozen policy)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(states)\n",
    "        return q_values.argmax(dim=1)\n",
    "\n",
    "def train_step(policy_net, target_net, optimizer, replay_buffer):\n",
    "    \"\"\"Perform one DQN training step\"\"\"\n",
    "    if not replay_buffer.is_ready(BATCH_SIZE):\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "    \n",
    "    states = states.to(device)\n",
    "    actions = actions.to(device)\n",
    "    rewards = rewards.to(device)\n",
    "    next_states = next_states.to(device)\n",
    "    dones = dones.to(device)\n",
    "    \n",
    "    # Compute Q values\n",
    "    q_values = policy_net(states)\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # Compute target Q values\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states).max(dim=1)[0]\n",
    "        target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = nn.functional.mse_loss(q_values, target_q_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def store_transitions(buffer, states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Store transitions in replay buffer\"\"\"\n",
    "    states_np = states.cpu().numpy()\n",
    "    actions_np = actions.cpu().numpy()\n",
    "    rewards_np = rewards.cpu().numpy()\n",
    "    next_states_np = next_states.cpu().numpy()\n",
    "    dones_np = dones.cpu().numpy()\n",
    "    \n",
    "    for i in range(len(states_np)):\n",
    "        buffer.push(states_np[i], actions_np[i], rewards_np[i], next_states_np[i], dones_np[i])\n",
    "\n",
    "# Global tracking\n",
    "total_steps = 0\n",
    "total_rounds = 0\n",
    "round_winners = []\n",
    "all_scores1 = []\n",
    "all_scores2 = []\n",
    "all_losses = []\n",
    "all_win_rates = []\n",
    "stage_results = []\n",
    "\n",
    "# Epsilon schedulers (will be reset per stage)\n",
    "epsilon1 = EPSILON_START\n",
    "epsilon2 = EPSILON_START\n",
    "\n",
    "def calculate_win_rate(window=100):\n",
    "    if len(round_winners) < window:\n",
    "        window = len(round_winners)\n",
    "    if window == 0:\n",
    "        return 0.0\n",
    "    recent = round_winners[-window:]\n",
    "    return sum(1 for w in recent if w == 1) / window\n",
    "\n",
    "print(\"DQN helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Stage 0 - Static Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 0: Static Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE0_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE0_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE0_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage0_start = time.time()\n",
    "stage_steps = 0\n",
    "env.target_food = STAGE0_TARGET_FOOD\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "# Reset epsilon for this stage\n",
    "epsilon1 = EPSILON_START\n",
    "epsilon_decay1 = (EPSILON_START - EPSILON_END) / STAGE0_MIN_STEPS\n",
    "\n",
    "while True:\n",
    "    # Agent 1 uses epsilon-greedy\n",
    "    actions1 = select_action(policy_net1, obs1, epsilon1)\n",
    "    # Agent 2 uses static scripted agent\n",
    "    actions2 = scripted_agents['static'].select_action(env)\n",
    "    \n",
    "    next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "    \n",
    "    # Store transitions for agent 1\n",
    "    store_transitions(buffer1, obs1, actions1, r1, next_obs1, dones)\n",
    "    \n",
    "    # Track completed rounds\n",
    "    if dones.any():\n",
    "        num_done = len(info['done_envs'])\n",
    "        for i in range(num_done):\n",
    "            round_winners.append(int(info['winners'][i]))\n",
    "            total_rounds += 1\n",
    "            all_scores1.append(info['food_counts1'][i])\n",
    "            all_scores2.append(info['food_counts2'][i])\n",
    "    \n",
    "    obs1 = next_obs1\n",
    "    obs2 = next_obs2\n",
    "    stage_steps += NUM_ENVS\n",
    "    total_steps += NUM_ENVS\n",
    "    \n",
    "    # Training\n",
    "    if stage_steps % max(1, int(1 / TRAIN_STEPS_RATIO)) == 0:\n",
    "        loss = train_step(policy_net1, target_net1, optimizer1, buffer1)\n",
    "        if loss is not None:\n",
    "            all_losses.append(loss)\n",
    "    \n",
    "    # Update target network\n",
    "    if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon1 = max(EPSILON_END, epsilon1 - epsilon_decay1 * NUM_ENVS)\n",
    "    \n",
    "    # Logging\n",
    "    if total_steps % LOG_INTERVAL == 0:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        avg_loss = np.mean(all_losses[-10:]) if all_losses else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {avg_loss:.4f} | Eps: {epsilon1:.3f}\")\n",
    "    \n",
    "    # Check stage completion\n",
    "    if stage_steps >= STAGE0_MIN_STEPS and calculate_win_rate() >= STAGE0_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE0_MIN_STEPS * 3:  # Safety limit\n",
    "        print(\"Warning: Stage 0 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage0_time = time.time() - stage0_start\n",
    "stage0_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 0,\n",
    "    'name': 'Static',\n",
    "    'time': stage0_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage0_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 0 COMPLETE\")\n",
    "print(f\"Time: {stage0_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage0_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Stage 1 - Random Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 1: Random Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE1_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE1_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE1_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage1_start = time.time()\n",
    "stage_steps = 0\n",
    "env.target_food = STAGE1_TARGET_FOOD\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "# Reset epsilon for this stage\n",
    "epsilon1 = EPSILON_START\n",
    "epsilon_decay1 = (EPSILON_START - EPSILON_END) / STAGE1_MIN_STEPS\n",
    "\n",
    "while True:\n",
    "    actions1 = select_action(policy_net1, obs1, epsilon1)\n",
    "    actions2 = scripted_agents['random'].select_action(env)\n",
    "    \n",
    "    next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "    \n",
    "    store_transitions(buffer1, obs1, actions1, r1, next_obs1, dones)\n",
    "    \n",
    "    if dones.any():\n",
    "        num_done = len(info['done_envs'])\n",
    "        for i in range(num_done):\n",
    "            round_winners.append(int(info['winners'][i]))\n",
    "            total_rounds += 1\n",
    "            all_scores1.append(info['food_counts1'][i])\n",
    "            all_scores2.append(info['food_counts2'][i])\n",
    "    \n",
    "    obs1 = next_obs1\n",
    "    obs2 = next_obs2\n",
    "    stage_steps += NUM_ENVS\n",
    "    total_steps += NUM_ENVS\n",
    "    \n",
    "    if stage_steps % max(1, int(1 / TRAIN_STEPS_RATIO)) == 0:\n",
    "        loss = train_step(policy_net1, target_net1, optimizer1, buffer1)\n",
    "        if loss is not None:\n",
    "            all_losses.append(loss)\n",
    "    \n",
    "    if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "    \n",
    "    epsilon1 = max(EPSILON_END, epsilon1 - epsilon_decay1 * NUM_ENVS)\n",
    "    \n",
    "    if total_steps % LOG_INTERVAL == 0:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        avg_loss = np.mean(all_losses[-10:]) if all_losses else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {avg_loss:.4f} | Eps: {epsilon1:.3f}\")\n",
    "    \n",
    "    if stage_steps >= STAGE1_MIN_STEPS and calculate_win_rate() >= STAGE1_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE1_MIN_STEPS * 3:\n",
    "        print(\"Warning: Stage 1 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage1_time = time.time() - stage1_start\n",
    "stage1_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 1,\n",
    "    'name': 'Random',\n",
    "    'time': stage1_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage1_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 1 COMPLETE\")\n",
    "print(f\"Time: {stage1_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage1_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Stage 2 - Greedy Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 2: Greedy Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE2_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE2_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE2_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage2_start = time.time()\n",
    "stage_steps = 0\n",
    "env.target_food = STAGE2_TARGET_FOOD\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "# Reset epsilon for this stage\n",
    "epsilon1 = EPSILON_START\n",
    "epsilon_decay1 = (EPSILON_START - EPSILON_END) / STAGE2_MIN_STEPS\n",
    "\n",
    "while True:\n",
    "    actions1 = select_action(policy_net1, obs1, epsilon1)\n",
    "    actions2 = scripted_agents['greedy'].select_action(env)\n",
    "    \n",
    "    next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "    \n",
    "    store_transitions(buffer1, obs1, actions1, r1, next_obs1, dones)\n",
    "    \n",
    "    if dones.any():\n",
    "        num_done = len(info['done_envs'])\n",
    "        for i in range(num_done):\n",
    "            round_winners.append(int(info['winners'][i]))\n",
    "            total_rounds += 1\n",
    "            all_scores1.append(info['food_counts1'][i])\n",
    "            all_scores2.append(info['food_counts2'][i])\n",
    "    \n",
    "    obs1 = next_obs1\n",
    "    obs2 = next_obs2\n",
    "    stage_steps += NUM_ENVS\n",
    "    total_steps += NUM_ENVS\n",
    "    \n",
    "    if stage_steps % max(1, int(1 / TRAIN_STEPS_RATIO)) == 0:\n",
    "        loss = train_step(policy_net1, target_net1, optimizer1, buffer1)\n",
    "        if loss is not None:\n",
    "            all_losses.append(loss)\n",
    "    \n",
    "    if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "    \n",
    "    epsilon1 = max(EPSILON_END, epsilon1 - epsilon_decay1 * NUM_ENVS)\n",
    "    \n",
    "    if total_steps % LOG_INTERVAL == 0:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        avg_loss = np.mean(all_losses[-10:]) if all_losses else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {avg_loss:.4f} | Eps: {epsilon1:.3f}\")\n",
    "    \n",
    "    if stage_steps >= STAGE2_MIN_STEPS and calculate_win_rate() >= STAGE2_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE2_MIN_STEPS * 3:\n",
    "        print(\"Warning: Stage 2 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage2_time = time.time() - stage2_start\n",
    "stage2_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 2,\n",
    "    'name': 'Greedy',\n",
    "    'time': stage2_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage2_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 2 COMPLETE\")\n",
    "print(f\"Time: {stage2_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage2_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Stage 3 - Frozen Policy Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 3: Frozen Policy Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE3_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE3_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE3_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage3_start = time.time()\n",
    "stage_steps = 0\n",
    "env.target_food = STAGE3_TARGET_FOOD\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "# Reset epsilon for this stage\n",
    "epsilon1 = EPSILON_START\n",
    "epsilon_decay1 = (EPSILON_START - EPSILON_END) / STAGE3_MIN_STEPS\n",
    "\n",
    "while True:\n",
    "    actions1 = select_action(policy_net1, obs1, epsilon1)\n",
    "    # Frozen policy: use agent2's policy with greedy selection\n",
    "    actions2 = select_greedy_action(policy_net2, obs2)\n",
    "    \n",
    "    next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "    \n",
    "    store_transitions(buffer1, obs1, actions1, r1, next_obs1, dones)\n",
    "    \n",
    "    if dones.any():\n",
    "        num_done = len(info['done_envs'])\n",
    "        for i in range(num_done):\n",
    "            round_winners.append(int(info['winners'][i]))\n",
    "            total_rounds += 1\n",
    "            all_scores1.append(info['food_counts1'][i])\n",
    "            all_scores2.append(info['food_counts2'][i])\n",
    "    \n",
    "    obs1 = next_obs1\n",
    "    obs2 = next_obs2\n",
    "    stage_steps += NUM_ENVS\n",
    "    total_steps += NUM_ENVS\n",
    "    \n",
    "    if stage_steps % max(1, int(1 / TRAIN_STEPS_RATIO)) == 0:\n",
    "        loss = train_step(policy_net1, target_net1, optimizer1, buffer1)\n",
    "        if loss is not None:\n",
    "            all_losses.append(loss)\n",
    "    \n",
    "    if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "    \n",
    "    epsilon1 = max(EPSILON_END, epsilon1 - epsilon_decay1 * NUM_ENVS)\n",
    "    \n",
    "    if total_steps % LOG_INTERVAL == 0:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        avg_loss = np.mean(all_losses[-10:]) if all_losses else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {avg_loss:.4f} | Eps: {epsilon1:.3f}\")\n",
    "    \n",
    "    if stage_steps >= STAGE3_MIN_STEPS and calculate_win_rate() >= STAGE3_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE3_MIN_STEPS * 3:\n",
    "        print(\"Warning: Stage 3 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage3_time = time.time() - stage3_start\n",
    "stage3_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 3,\n",
    "    'name': 'Frozen',\n",
    "    'time': stage3_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage3_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 3 COMPLETE\")\n",
    "print(f\"Time: {stage3_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage3_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Stage 4 - Co-Evolution (Both Learning)\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 4: Co-Evolution (Both Learning)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE4_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE4_MIN_STEPS}\")\n",
    "print(\"Win rate threshold: None (train for full duration)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage4_start = time.time()\n",
    "stage_steps = 0\n",
    "env.target_food = STAGE4_TARGET_FOOD\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "# Reset epsilon for both agents\n",
    "epsilon1 = EPSILON_START\n",
    "epsilon2 = EPSILON_START\n",
    "epsilon_decay1 = (EPSILON_START - EPSILON_END) / STAGE4_MIN_STEPS\n",
    "epsilon_decay2 = (EPSILON_START - EPSILON_END) / STAGE4_MIN_STEPS\n",
    "\n",
    "while stage_steps < STAGE4_MIN_STEPS:\n",
    "    # Both agents learning with epsilon-greedy\n",
    "    actions1 = select_action(policy_net1, obs1, epsilon1)\n",
    "    actions2 = select_action(policy_net2, obs2, epsilon2)\n",
    "    \n",
    "    next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "    \n",
    "    # Store transitions for both agents\n",
    "    store_transitions(buffer1, obs1, actions1, r1, next_obs1, dones)\n",
    "    store_transitions(buffer2, obs2, actions2, r2, next_obs2, dones)\n",
    "    \n",
    "    if dones.any():\n",
    "        num_done = len(info['done_envs'])\n",
    "        for i in range(num_done):\n",
    "            round_winners.append(int(info['winners'][i]))\n",
    "            total_rounds += 1\n",
    "            all_scores1.append(info['food_counts1'][i])\n",
    "            all_scores2.append(info['food_counts2'][i])\n",
    "    \n",
    "    obs1 = next_obs1\n",
    "    obs2 = next_obs2\n",
    "    stage_steps += NUM_ENVS\n",
    "    total_steps += NUM_ENVS\n",
    "    \n",
    "    # Train both agents\n",
    "    if stage_steps % max(1, int(1 / TRAIN_STEPS_RATIO)) == 0:\n",
    "        loss1 = train_step(policy_net1, target_net1, optimizer1, buffer1)\n",
    "        loss2 = train_step(policy_net2, target_net2, optimizer2, buffer2)\n",
    "        if loss1 is not None:\n",
    "            all_losses.append(loss1)\n",
    "    \n",
    "    # Update target networks\n",
    "    if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "        target_net2.load_state_dict(policy_net2.state_dict())\n",
    "    \n",
    "    # Decay epsilon for both\n",
    "    epsilon1 = max(EPSILON_END, epsilon1 - epsilon_decay1 * NUM_ENVS)\n",
    "    epsilon2 = max(EPSILON_END, epsilon2 - epsilon_decay2 * NUM_ENVS)\n",
    "    \n",
    "    if total_steps % LOG_INTERVAL == 0:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        avg_score2 = np.mean(all_scores2[-100:]) if all_scores2 else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Scores: {avg_score1:.1f} vs {avg_score2:.1f}\")\n",
    "\n",
    "stage4_time = time.time() - stage4_start\n",
    "stage4_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 4,\n",
    "    'name': 'Co-Evolution',\n",
    "    'time': stage4_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage4_win_rate\n",
    "})\n",
    "\n",
    "total_training_time = sum(r['time'] for r in stage_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 4 COMPLETE\")\n",
    "print(f\"Time: {stage4_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage4_win_rate:.2%}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTOTAL CURRICULUM TRAINING COMPLETE!\")\n",
    "print(f\"Total time: {total_training_time:.1f}s ({total_training_time/60:.1f} min)\")\n",
    "print(f\"Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Weights\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save Agent 1 (Big)\n",
    "agent1_path = save_dir / f\"big_256x256_curriculum_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    'policy_net': policy_net1.state_dict(),\n",
    "    'target_net': target_net1.state_dict(),\n",
    "    'optimizer': optimizer1.state_dict(),\n",
    "    'total_steps': total_steps,\n",
    "    'config': {\n",
    "        'hidden_dims': BIG_HIDDEN_DIMS,\n",
    "        'grid_size': GRID_SIZE,\n",
    "        'learning_rate': LEARNING_RATE\n",
    "    }\n",
    "}, agent1_path)\n",
    "print(f\"Agent 1 (Big) saved to: {agent1_path}\")\n",
    "\n",
    "# Save Agent 2 (Small)\n",
    "agent2_path = save_dir / f\"small_128x128_curriculum_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    'policy_net': policy_net2.state_dict(),\n",
    "    'target_net': target_net2.state_dict(),\n",
    "    'optimizer': optimizer2.state_dict(),\n",
    "    'total_steps': total_steps,\n",
    "    'config': {\n",
    "        'hidden_dims': SMALL_HIDDEN_DIMS,\n",
    "        'grid_size': GRID_SIZE,\n",
    "        'learning_rate': LEARNING_RATE\n",
    "    }\n",
    "}, agent2_path)\n",
    "print(f\"Agent 2 (Small) saved to: {agent2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "def smooth(data, window=50):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot 1: Agent Scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(all_scores1, alpha=0.3, label='Agent 1 (Big)', color='blue')\n",
    "ax1.plot(all_scores2, alpha=0.3, label='Agent 2 (Small)', color='red')\n",
    "if len(all_scores1) > 50:\n",
    "    ax1.plot(range(49, len(all_scores1)), smooth(all_scores1), color='blue', linewidth=2)\n",
    "    ax1.plot(range(49, len(all_scores2)), smooth(all_scores2), color='red', linewidth=2)\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Score (Food Eaten)')\n",
    "ax1.set_title('Agent Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Win Rate Over Time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(all_win_rates, color='green')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', label='50% (balanced)')\n",
    "ax2.set_xlabel('Training Progress')\n",
    "ax2.set_ylabel('Agent 1 Win Rate')\n",
    "ax2.set_title('Win Rate Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Loss\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(all_losses, alpha=0.5)\n",
    "if len(all_losses) > 50:\n",
    "    ax3.plot(range(49, len(all_losses)), smooth(all_losses), color='blue', linewidth=2)\n",
    "ax3.set_xlabel('Update')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('DQN Loss')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Stage Results\n",
    "ax4 = axes[1, 1]\n",
    "stage_names = [r['name'] for r in stage_results]\n",
    "stage_win_rates = [r['win_rate'] * 100 for r in stage_results]\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "bars = ax4.bar(stage_names, stage_win_rates, color=colors[:len(stage_results)])\n",
    "ax4.set_xlabel('Stage')\n",
    "ax4.set_ylabel('Final Win Rate (%)')\n",
    "ax4.set_title('Win Rate by Curriculum Stage')\n",
    "ax4.set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, stage_win_rates):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{rate:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'DQN Two-Snake Curriculum Training Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = project_root / 'results' / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig_path = fig_dir / f'dqn_two_snake_curriculum_{timestamp}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Results Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"CURRICULUM TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Algorithm: DQN Two-Snake Curriculum\")\n",
    "print(f\"Grid Size: {GRID_SIZE}x{GRID_SIZE}\")\n",
    "print(f\"Total Steps: {total_steps:,}\")\n",
    "print(f\"Total Rounds: {total_rounds:,}\")\n",
    "print(f\"Total Training Time: {total_training_time:.1f}s ({total_training_time/60:.1f} min)\")\n",
    "print()\n",
    "print(\"Stage Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Stage':<15} {'Opponent':<15} {'Steps':>10} {'Time':>10} {'Win Rate':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for r in stage_results:\n",
    "    print(f\"Stage {r['stage']:<8} {r['name']:<15} {r['steps']:>10,} {r['time']:>9.1f}s {r['win_rate']:>11.2%}\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Final Performance (last 100 rounds):\")\n",
    "print(f\"  Agent 1 Avg Score: {np.mean(all_scores1[-100:]):.2f}\")\n",
    "print(f\"  Agent 2 Avg Score: {np.mean(all_scores2[-100:]):.2f}\")\n",
    "print(f\"  Final Win Rate: {calculate_win_rate():.2%}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
