{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Snake Classic DQN Training Notebook\n",
    "\n",
    "This notebook trains two competing DQN agents to play against each other.\n",
    "\n",
    "**Algorithm**: Classic DQN Two-Snake Training with:\n",
    "- Two independent DQN agents competing\n",
    "- Episode-based training (not vectorized)\n",
    "- Each agent has its own Q-network and replay buffer\n",
    "- Target: reach target_score food items first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from core.environment_two_snake_classic import TwoSnakeCompetitiveEnv\n",
    "from agents.vanilla_dqn import VanillaDQNAgent\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (papermill parameters)\n",
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Environment\n",
    "GRID_SIZE = 10\n",
    "TARGET_SCORE = 10  # Food needed to win\n",
    "MAX_STEPS_PER_EPISODE = 500\n",
    "\n",
    "# Training\n",
    "NUM_EPISODES = 500  # Short for testing (full: 10000)\n",
    "EVAL_FREQ = 50  # Logging frequency\n",
    "SAVE_FREQ = 500  # Checkpoint frequency\n",
    "\n",
    "# DQN Agent Config\n",
    "HIDDEN_SIZE = 128\n",
    "LEARNING_RATE = 0.0005\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "\n",
    "# Output\n",
    "SAVE_DIR = '../../results/weights/competitive/classic'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Environment & Agent Setup\n",
    "np.random.seed(SEED)\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create environment\n",
    "env = TwoSnakeCompetitiveEnv(\n",
    "    grid_size=GRID_SIZE,\n",
    "    target_score=TARGET_SCORE,\n",
    "    max_steps=MAX_STEPS_PER_EPISODE\n",
    ")\n",
    "\n",
    "state_size = env.observation_space.shape[0]  # 20 features\n",
    "action_size = env.action_space.n  # 3 actions\n",
    "\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "\n",
    "# Create Agent 1\n",
    "agent1 = VanillaDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    epsilon=EPSILON_START,\n",
    "    epsilon_min=EPSILON_MIN,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_target_network=True,\n",
    "    target_update_freq=TARGET_UPDATE_FREQ\n",
    ")\n",
    "\n",
    "# Create Agent 2\n",
    "agent2 = VanillaDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    epsilon=EPSILON_START,\n",
    "    epsilon_min=EPSILON_MIN,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_target_network=True,\n",
    "    target_update_freq=TARGET_UPDATE_FREQ\n",
    ")\n",
    "\n",
    "print(f\"Environment: {GRID_SIZE}x{GRID_SIZE} grid\")\n",
    "print(f\"Target score: {TARGET_SCORE} food items\")\n",
    "print(f\"Agent 1: DQN with {HIDDEN_SIZE} hidden neurons\")\n",
    "print(f\"Agent 2: DQN with {HIDDEN_SIZE} hidden neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Training Loop\nprint(\"Starting Two-Snake Classic DQN Training...\")\nprint(f\"Episodes: {NUM_EPISODES}\")\nprint()\n\nstart_time = time.time()\n\n# Statistics tracking\nstats = {\n    'agent1_wins': 0,\n    'agent2_wins': 0,\n    'draws': 0,\n    'agent1_total_reward': 0.0,\n    'agent2_total_reward': 0.0,\n}\n\n# For plotting\nall_agent1_rewards = []\nall_agent2_rewards = []\nall_agent1_scores = []\nall_agent2_scores = []\nall_win_rates = []\ntotal_steps = 0\n\nfor episode in range(1, NUM_EPISODES + 1):\n    observations, _ = env.reset()\n    state1 = observations['agent1']\n    state2 = observations['agent2']\n\n    episode_reward1 = 0\n    episode_reward2 = 0\n    done = False\n    step = 0\n\n    while not done and step < MAX_STEPS_PER_EPISODE:\n        # Get actions from both agents\n        action1 = agent1.select_action(state1, training=True)\n        action2 = agent2.select_action(state2, training=True)\n\n        # Step environment (dict-based API)\n        actions = {'agent1': action1, 'agent2': action2}\n        next_observations, rewards, terminated, truncated, info = env.step(actions)\n\n        next_state1 = next_observations['agent1']\n        next_state2 = next_observations['agent2']\n        reward1 = rewards['agent1']\n        reward2 = rewards['agent2']\n        done1 = terminated['agent1'] or truncated['agent1']\n        done2 = terminated['agent2'] or truncated['agent2']\n        done = done1 or done2\n\n        # Train both agents\n        agent1.train_step(state1, action1, reward1, next_state1, done1)\n        agent2.train_step(state2, action2, reward2, next_state2, done2)\n\n        episode_reward1 += reward1\n        episode_reward2 += reward2\n\n        state1 = next_state1\n        state2 = next_state2\n        step += 1\n        total_steps += 1\n\n    # Decay epsilon\n    agent1.decay_epsilon()\n    agent2.decay_epsilon()\n\n    # Track winner\n    if 'winner' in info:\n        if info['winner'] == 1:\n            stats['agent1_wins'] += 1\n        elif info['winner'] == 2:\n            stats['agent2_wins'] += 1\n        else:\n            stats['draws'] += 1\n    else:\n        stats['draws'] += 1\n\n    # Store metrics\n    all_agent1_rewards.append(episode_reward1)\n    all_agent2_rewards.append(episode_reward2)\n    all_agent1_scores.append(info.get('score1', 0))\n    all_agent2_scores.append(info.get('score2', 0))\n\n    win_rate1 = stats['agent1_wins'] / episode if episode > 0 else 0\n    all_win_rates.append(win_rate1)\n\n    # Logging\n    if episode % EVAL_FREQ == 0:\n        elapsed = time.time() - start_time\n        eps_per_sec = episode / elapsed if elapsed > 0 else 0\n\n        # Recent window stats\n        window = min(100, episode)\n        recent_a1_scores = all_agent1_scores[-window:]\n        recent_a2_scores = all_agent2_scores[-window:]\n\n        print(f\"Episode {episode}/{NUM_EPISODES} | \"\n              f\"A1 Score: {np.mean(recent_a1_scores):.2f} | \"\n              f\"A2 Score: {np.mean(recent_a2_scores):.2f} | \"\n              f\"Win Rate: {win_rate1:.2%} | \"\n              f\"Eps: {agent1.epsilon:.3f} | \"\n              f\"Speed: {eps_per_sec:.1f} ep/s\")\n\ntraining_time = time.time() - start_time\nprint(f\"\\nTraining complete!\")\nprint(f\"Total time: {training_time:.1f}s\")\nprint(f\"Total steps: {total_steps:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save Weights\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save Agent 1\n",
    "agent1_path = save_dir / f\"agent1_dqn_{NUM_EPISODES}ep_{timestamp}.pt\"\n",
    "agent1.save(str(agent1_path))\n",
    "print(f\"Agent 1 saved to: {agent1_path}\")\n",
    "\n",
    "# Save Agent 2\n",
    "agent2_path = save_dir / f\"agent2_dqn_{NUM_EPISODES}ep_{timestamp}.pt\"\n",
    "agent2.save(str(agent2_path))\n",
    "print(f\"Agent 2 saved to: {agent2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "def smooth(data, window=50):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot 1: Agent Scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(all_agent1_scores, alpha=0.3, label='Agent 1 (raw)', color='blue')\n",
    "ax1.plot(all_agent2_scores, alpha=0.3, label='Agent 2 (raw)', color='red')\n",
    "if len(all_agent1_scores) > 50:\n",
    "    ax1.plot(range(49, len(all_agent1_scores)), smooth(all_agent1_scores), label='Agent 1 (smooth)', color='blue')\n",
    "    ax1.plot(range(49, len(all_agent2_scores)), smooth(all_agent2_scores), label='Agent 2 (smooth)', color='red')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Score (Food Eaten)')\n",
    "ax1.set_title('Agent Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Agent Rewards\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(all_agent1_rewards, alpha=0.3, label='Agent 1', color='blue')\n",
    "ax2.plot(all_agent2_rewards, alpha=0.3, label='Agent 2', color='red')\n",
    "if len(all_agent1_rewards) > 50:\n",
    "    ax2.plot(range(49, len(all_agent1_rewards)), smooth(all_agent1_rewards), color='blue')\n",
    "    ax2.plot(range(49, len(all_agent2_rewards)), smooth(all_agent2_rewards), color='red')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Reward')\n",
    "ax2.set_title('Agent Rewards')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Win Rate\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(all_win_rates, color='green')\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', label='50% (balanced)')\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Agent 1 Win Rate')\n",
    "ax3.set_title('Cumulative Win Rate')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Win Distribution\n",
    "ax4 = axes[1, 1]\n",
    "labels = ['Agent 1 Wins', 'Agent 2 Wins', 'Draws']\n",
    "sizes = [stats['agent1_wins'], stats['agent2_wins'], stats['draws']]\n",
    "colors = ['blue', 'red', 'gray']\n",
    "ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax4.set_title('Match Outcome Distribution')\n",
    "\n",
    "plt.suptitle(f'Two-Snake Classic DQN Training - {NUM_EPISODES} Episodes', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = project_root / 'results' / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig_path = fig_dir / f'two_snake_classic_training_{timestamp}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Algorithm: Classic DQN Two-Snake Competitive\")\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Total Steps: {total_steps:,}\")\n",
    "print(f\"Training Time: {training_time:.1f}s\")\n",
    "print()\n",
    "print(\"Match Results:\")\n",
    "print(f\"  Agent 1 Wins: {stats['agent1_wins']} ({stats['agent1_wins']/NUM_EPISODES*100:.1f}%)\")\n",
    "print(f\"  Agent 2 Wins: {stats['agent2_wins']} ({stats['agent2_wins']/NUM_EPISODES*100:.1f}%)\")\n",
    "print(f\"  Draws: {stats['draws']} ({stats['draws']/NUM_EPISODES*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Final Performance (last 100 episodes):\")\n",
    "window = min(100, NUM_EPISODES)\n",
    "print(f\"  Agent 1 Avg Score: {np.mean(all_agent1_scores[-window:]):.2f}\")\n",
    "print(f\"  Agent 2 Avg Score: {np.mean(all_agent2_scores[-window:]):.2f}\")\n",
    "print(f\"  Agent 1 Avg Reward: {np.mean(all_agent1_rewards[-window:]):.2f}\")\n",
    "print(f\"  Agent 2 Avg Reward: {np.mean(all_agent2_rewards[-window:]):.2f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}