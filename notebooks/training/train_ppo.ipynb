{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training Notebook\n",
    "\n",
    "This notebook trains a PPO (Proximal Policy Optimization) agent to play Snake.\n",
    "\n",
    "**Algorithm**: PPO with:\n",
    "- Actor-Critic architecture (separate networks)\n",
    "- Clipped surrogate objective\n",
    "- Generalized Advantage Estimation (GAE)\n",
    "- Multiple epochs per rollout\n",
    "- Entropy bonus for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from core.environment_vectorized import VectorizedSnakeEnv\n",
    "from core.networks import PPO_Actor_MLP, PPO_Critic_MLP, PPO_Actor_CNN, PPO_Critic_CNN\n",
    "from core.utils import MetricsTracker, set_seed, get_device\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (papermill parameters)\n",
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Environment\n",
    "GRID_SIZE = 10\n",
    "NUM_ENVS = 256\n",
    "MAX_STEPS = 1000\n",
    "ACTION_SPACE_TYPE = 'relative'  # 'absolute' or 'relative'\n",
    "STATE_REPRESENTATION = 'feature'  # 'feature' or 'grid'\n",
    "USE_FLOOD_FILL = False\n",
    "\n",
    "# Training\n",
    "NUM_EPISODES = 500  # Short for testing\n",
    "ROLLOUT_STEPS = 2048  # Steps before each PPO update\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_PER_ROLLOUT = 4  # PPO epochs per collected rollout\n",
    "\n",
    "# Learning rates\n",
    "ACTOR_LR = 0.0003\n",
    "CRITIC_LR = 0.001\n",
    "\n",
    "# PPO hyperparameters\n",
    "GAMMA = 0.99  # Discount factor\n",
    "GAE_LAMBDA = 0.95  # GAE parameter\n",
    "CLIP_EPSILON = 0.2  # PPO clipping range\n",
    "VALUE_LOSS_COEF = 0.5  # Value loss coefficient\n",
    "ENTROPY_COEF = 0.01  # Entropy bonus coefficient\n",
    "MAX_GRAD_NORM = 0.5  # Gradient clipping\n",
    "\n",
    "# Network\n",
    "HIDDEN_DIMS = (128, 128)\n",
    "\n",
    "# Output\n",
    "SAVE_DIR = '../../results/weights/ppo'\n",
    "SEED = 42\n",
    "LOG_INTERVAL = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Environment & Model Setup\n",
    "set_seed(SEED)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create environment\n",
    "env = VectorizedSnakeEnv(\n",
    "    num_envs=NUM_ENVS,\n",
    "    grid_size=GRID_SIZE,\n",
    "    action_space_type=ACTION_SPACE_TYPE,\n",
    "    state_representation=STATE_REPRESENTATION,\n",
    "    max_steps=MAX_STEPS,\n",
    "    use_flood_fill=USE_FLOOD_FILL,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Determine dimensions\n",
    "if STATE_REPRESENTATION == 'feature':\n",
    "    input_dim = 14 if USE_FLOOD_FILL else 11\n",
    "else:\n",
    "    input_dim = GRID_SIZE\n",
    "\n",
    "output_dim = 3 if ACTION_SPACE_TYPE == 'relative' else 4\n",
    "\n",
    "# Create Actor-Critic networks\n",
    "if STATE_REPRESENTATION == 'feature':\n",
    "    actor = PPO_Actor_MLP(input_dim, output_dim, HIDDEN_DIMS).to(device)\n",
    "    critic = PPO_Critic_MLP(input_dim, HIDDEN_DIMS).to(device)\n",
    "else:\n",
    "    actor = PPO_Actor_CNN(GRID_SIZE, 3, output_dim).to(device)\n",
    "    critic = PPO_Critic_CNN(GRID_SIZE, 3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "# Metrics\n",
    "metrics = MetricsTracker(window_size=100)\n",
    "\n",
    "print(f\"Environment: {GRID_SIZE}x{GRID_SIZE} grid, {NUM_ENVS} parallel envs\")\n",
    "print(f\"Actor parameters: {sum(p.numel() for p in actor.parameters())}\")\n",
    "print(f\"Critic parameters: {sum(p.numel() for p in critic.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PPO Helper Functions\n",
    "\n",
    "class PPOBuffer:\n",
    "    \"\"\"Rollout buffer for on-policy PPO training\"\"\"\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.size = 0\n",
    "    \n",
    "    def add(self, state, action, reward, done, log_prob, value):\n",
    "        self.states.append(state.cpu())\n",
    "        self.actions.append(action.cpu())\n",
    "        self.rewards.append(reward.cpu())\n",
    "        self.dones.append(done.cpu())\n",
    "        self.log_probs.append(log_prob.cpu())\n",
    "        self.values.append(value.cpu())\n",
    "        self.size += state.shape[0]\n",
    "    \n",
    "    def get(self):\n",
    "        states = torch.cat(self.states, dim=0).to(self.device)\n",
    "        actions = torch.cat(self.actions, dim=0).to(self.device)\n",
    "        rewards = torch.cat(self.rewards, dim=0).to(self.device)\n",
    "        dones = torch.cat(self.dones, dim=0).to(self.device)\n",
    "        log_probs = torch.cat(self.log_probs, dim=0).to(self.device)\n",
    "        values = torch.cat(self.values, dim=0).to(self.device)\n",
    "        return states, actions, rewards, dones, log_probs, values\n",
    "\n",
    "def select_actions(states):\n",
    "    \"\"\"Select actions using current policy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = actor(states)\n",
    "        values = critic(states)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "    return actions, log_probs, values\n",
    "\n",
    "def compute_gae(rewards, values, dones, next_value):\n",
    "    \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "    dones = dones.float()\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            next_non_terminal = 1.0 - dones[t]\n",
    "            next_value_t = next_value\n",
    "        else:\n",
    "            next_non_terminal = 1.0 - dones[t]\n",
    "            next_value_t = values[t + 1]\n",
    "        \n",
    "        delta = rewards[t] + GAMMA * next_value_t * next_non_terminal - values[t]\n",
    "        gae = delta + GAMMA * GAE_LAMBDA * next_non_terminal * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = torch.stack(advantages)\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "def ppo_update(states, actions, old_log_probs, advantages, returns):\n",
    "    \"\"\"PPO update step\"\"\"\n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    total_actor_loss = 0\n",
    "    total_critic_loss = 0\n",
    "    total_entropy = 0\n",
    "    n_updates = 0\n",
    "    \n",
    "    for _ in range(EPOCHS_PER_ROLLOUT):\n",
    "        indices = torch.randperm(states.size(0))\n",
    "        \n",
    "        for start in range(0, states.size(0), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            batch_idx = indices[start:end]\n",
    "            \n",
    "            batch_states = states[batch_idx]\n",
    "            batch_actions = actions[batch_idx]\n",
    "            batch_old_log_probs = old_log_probs[batch_idx]\n",
    "            batch_advantages = advantages[batch_idx]\n",
    "            batch_returns = returns[batch_idx]\n",
    "            \n",
    "            # Evaluate actions\n",
    "            logits = actor(batch_states)\n",
    "            values = critic(batch_states)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            log_probs = dist.log_prob(batch_actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # PPO clipped objective\n",
    "            ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "            surr1 = ratio * batch_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * batch_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = F.mse_loss(values.squeeze(), batch_returns)\n",
    "            \n",
    "            # Update actor\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(actor.parameters(), MAX_GRAD_NORM)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # Update critic\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), MAX_GRAD_NORM)\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            total_actor_loss += actor_loss.item()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "            total_entropy += entropy.item()\n",
    "            n_updates += 1\n",
    "    \n",
    "    return total_actor_loss / n_updates, total_critic_loss / n_updates, total_entropy / n_updates\n",
    "\n",
    "print(\"PPO helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Loop\n",
    "import time\n",
    "\n",
    "print(\"Starting PPO Training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Rollout steps: {ROLLOUT_STEPS}\")\n",
    "print()\n",
    "\n",
    "buffer = PPOBuffer(ROLLOUT_STEPS, device)\n",
    "states = env.reset(seed=SEED)\n",
    "start_time = time.time()\n",
    "\n",
    "episode_rewards = torch.zeros(NUM_ENVS, device=device)\n",
    "episode_lengths = torch.zeros(NUM_ENVS, device=device)\n",
    "\n",
    "episode = 0\n",
    "total_steps = 0\n",
    "\n",
    "# Tracking for plots\n",
    "all_rewards = []\n",
    "all_scores = []\n",
    "all_actor_losses = []\n",
    "all_critic_losses = []\n",
    "all_entropies = []\n",
    "\n",
    "while episode < NUM_EPISODES:\n",
    "    # Collect rollout\n",
    "    buffer.clear()\n",
    "    \n",
    "    for _ in range(max(1, ROLLOUT_STEPS // NUM_ENVS)):\n",
    "        actions, log_probs, values = select_actions(states)\n",
    "        next_states, rewards, dones, info = env.step(actions)\n",
    "        \n",
    "        buffer.add(states, actions, rewards, dones, log_probs, values.squeeze())\n",
    "        \n",
    "        episode_rewards += rewards\n",
    "        episode_lengths += 1\n",
    "        total_steps += NUM_ENVS\n",
    "        \n",
    "        if dones.any():\n",
    "            done_indices = torch.where(dones)[0]\n",
    "            for idx in done_indices:\n",
    "                reward = episode_rewards[idx].item()\n",
    "                length = episode_lengths[idx].item()\n",
    "                score = info['scores'][idx].item()\n",
    "                \n",
    "                metrics.add_episode(reward, length, score)\n",
    "                all_rewards.append(reward)\n",
    "                all_scores.append(score)\n",
    "                \n",
    "                episode += 1\n",
    "                episode_rewards[idx] = 0\n",
    "                episode_lengths[idx] = 0\n",
    "                \n",
    "                if episode % LOG_INTERVAL == 0:\n",
    "                    stats = metrics.get_recent_stats()\n",
    "                    elapsed = time.time() - start_time\n",
    "                    fps = total_steps / elapsed if elapsed > 0 else 0\n",
    "                    print(f\"Episode {episode}/{NUM_EPISODES} | \"\n",
    "                          f\"Score: {stats['avg_score']:.2f} | \"\n",
    "                          f\"Reward: {stats['avg_reward']:.2f} | \"\n",
    "                          f\"FPS: {fps:.0f}\")\n",
    "                \n",
    "                if episode >= NUM_EPISODES:\n",
    "                    break\n",
    "        \n",
    "        states = next_states\n",
    "        if episode >= NUM_EPISODES:\n",
    "            break\n",
    "    \n",
    "    if episode >= NUM_EPISODES:\n",
    "        break\n",
    "    \n",
    "    # Compute final value for bootstrapping\n",
    "    with torch.no_grad():\n",
    "        next_value = critic(states).squeeze()\n",
    "    \n",
    "    # Get buffer data\n",
    "    buf_states, buf_actions, buf_rewards, buf_dones, buf_log_probs, buf_values = buffer.get()\n",
    "    \n",
    "    # Reshape for GAE\n",
    "    steps_per_env = len(buffer.states)\n",
    "    buf_rewards = buf_rewards.view(steps_per_env, NUM_ENVS)\n",
    "    buf_values = buf_values.view(steps_per_env, NUM_ENVS)\n",
    "    buf_dones = buf_dones.view(steps_per_env, NUM_ENVS)\n",
    "    \n",
    "    # Compute advantages for each env\n",
    "    all_advantages = []\n",
    "    all_returns = []\n",
    "    for env_idx in range(NUM_ENVS):\n",
    "        adv, ret = compute_gae(\n",
    "            buf_rewards[:, env_idx],\n",
    "            buf_values[:, env_idx],\n",
    "            buf_dones[:, env_idx],\n",
    "            next_value[env_idx]\n",
    "        )\n",
    "        all_advantages.append(adv)\n",
    "        all_returns.append(ret)\n",
    "    \n",
    "    advantages = torch.stack(all_advantages, dim=1).view(-1)\n",
    "    returns = torch.stack(all_returns, dim=1).view(-1)\n",
    "    \n",
    "    # PPO update\n",
    "    actor_loss, critic_loss, entropy = ppo_update(\n",
    "        buf_states, buf_actions, buf_log_probs, advantages, returns\n",
    "    )\n",
    "    all_actor_losses.append(actor_loss)\n",
    "    all_critic_losses.append(critic_loss)\n",
    "    all_entropies.append(entropy)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Total time: {training_time:.1f}s\")\n",
    "print(f\"Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Save Weights\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"ppo_{GRID_SIZE}x{GRID_SIZE}_{NUM_EPISODES}ep_{timestamp}.pt\"\n",
    "filepath = save_dir / filename\n",
    "\n",
    "torch.save({\n",
    "    'actor': actor.state_dict(),\n",
    "    'critic': critic.state_dict(),\n",
    "    'actor_optimizer': actor_optimizer.state_dict(),\n",
    "    'critic_optimizer': critic_optimizer.state_dict(),\n",
    "    'episode': episode,\n",
    "    'total_steps': total_steps,\n",
    "    'config': {\n",
    "        'grid_size': GRID_SIZE,\n",
    "        'num_envs': NUM_ENVS,\n",
    "        'hidden_dims': HIDDEN_DIMS,\n",
    "        'actor_lr': ACTOR_LR,\n",
    "        'critic_lr': CRITIC_LR,\n",
    "        'gamma': GAMMA,\n",
    "        'gae_lambda': GAE_LAMBDA,\n",
    "        'clip_epsilon': CLIP_EPSILON,\n",
    "        'state_representation': STATE_REPRESENTATION\n",
    "    }\n",
    "}, filepath)\n",
    "\n",
    "print(f\"Model saved to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "def smooth(data, window=50):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(all_rewards, alpha=0.3, label='Raw')\n",
    "if len(all_rewards) > 50:\n",
    "    ax1.plot(range(49, len(all_rewards)), smooth(all_rewards), label='Smoothed (50)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Rewards')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Scores\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(all_scores, alpha=0.3, label='Raw')\n",
    "if len(all_scores) > 50:\n",
    "    ax2.plot(range(49, len(all_scores)), smooth(all_scores), label='Smoothed (50)')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Score (Food Eaten)')\n",
    "ax2.set_title('Episode Scores')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Actor and Critic Loss\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(all_actor_losses, label='Actor Loss', alpha=0.7)\n",
    "ax3.plot(all_critic_losses, label='Critic Loss', alpha=0.7)\n",
    "ax3.set_xlabel('PPO Update')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('PPO Losses')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Entropy\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(all_entropies)\n",
    "ax4.set_xlabel('PPO Update')\n",
    "ax4.set_ylabel('Entropy')\n",
    "ax4.set_title('Policy Entropy')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'PPO Training Results - {NUM_EPISODES} Episodes', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = project_root / 'results' / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig_path = fig_dir / f'ppo_training_{timestamp}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Results Summary\n",
    "stats = metrics.get_recent_stats()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Algorithm: PPO (Proximal Policy Optimization)\")\n",
    "print(f\"Episodes: {episode}\")\n",
    "print(f\"Total Steps: {total_steps:,}\")\n",
    "print(f\"Training Time: {training_time:.1f}s\")\n",
    "print()\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"  Clip Epsilon: {CLIP_EPSILON}\")\n",
    "print(f\"  GAE Lambda: {GAE_LAMBDA}\")\n",
    "print(f\"  Entropy Coef: {ENTROPY_COEF}\")\n",
    "print()\n",
    "print(\"Final Performance (last 100 episodes):\")\n",
    "print(f\"  Average Score: {stats['avg_score']:.2f}\")\n",
    "print(f\"  Average Reward: {stats['avg_reward']:.2f}\")\n",
    "print(f\"  Average Length: {stats['avg_length']:.2f}\")\n",
    "print(f\"  Max Score: {stats['max_score']}\")\n",
    "print()\n",
    "print(f\"Overall Statistics:\")\n",
    "print(f\"  Mean Score: {np.mean(all_scores):.2f} +/- {np.std(all_scores):.2f}\")\n",
    "print(f\"  Max Score: {max(all_scores)}\")\n",
    "print(f\"  Mean Reward: {np.mean(all_rewards):.2f} +/- {np.std(all_rewards):.2f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
