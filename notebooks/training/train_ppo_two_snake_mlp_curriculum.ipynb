{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PPO Two-Snake Curriculum Training Notebook (MLP)\n",
    "\n",
    "This notebook trains PPO agents using curriculum learning for two-snake competitive play.\n",
    "\n",
    "**Algorithm**: PPO with 5-stage curriculum:\n",
    "1. Stage 0: vs StaticAgent (learn basic movement, 70% win threshold)\n",
    "2. Stage 1: vs RandomAgent (handle unpredictability, 60% win threshold)\n",
    "3. Stage 2: vs GreedyFoodAgent (compete for food, 55% win threshold)\n",
    "4. Stage 3: vs Frozen small network (50% win threshold)\n",
    "5. Stage 4: Co-evolution (both learning, no threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "from core.environment_two_snake_vectorized import VectorizedTwoSnakeEnv\n",
    "from core.networks import PPO_Actor_MLP, PPO_Critic_MLP\n",
    "from core.utils import set_seed, get_device\n",
    "from scripts.baselines.scripted_opponents import get_scripted_agent\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "tags": ["parameters"]
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (papermill parameters)\n",
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Environment\n",
    "GRID_SIZE = 20\n",
    "NUM_ENVS = 128\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Network sizes\n",
    "BIG_HIDDEN_DIMS = (256, 256)  # Agent 1\n",
    "SMALL_HIDDEN_DIMS = (128, 128)  # Agent 2\n",
    "\n",
    "# PPO hyperparameters\n",
    "ACTOR_LR = 0.0003\n",
    "CRITIC_LR = 0.0003\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "PPO_EPOCHS = 4\n",
    "BATCH_SIZE = 64\n",
    "ROLLOUT_STEPS = 256  # Steps per rollout\n",
    "\n",
    "# Curriculum Stage Settings (min_steps for each)\n",
    "STAGE0_MIN_STEPS = 5000  # Static opponent\n",
    "STAGE1_MIN_STEPS = 7500  # Random opponent\n",
    "STAGE2_MIN_STEPS = 10000  # Greedy opponent\n",
    "STAGE3_MIN_STEPS = 10000  # Frozen policy\n",
    "STAGE4_MIN_STEPS = 20000  # Co-evolution\n",
    "\n",
    "# Curriculum thresholds\n",
    "STAGE0_WIN_THRESHOLD = 0.70\n",
    "STAGE1_WIN_THRESHOLD = 0.60\n",
    "STAGE2_WIN_THRESHOLD = 0.55\n",
    "STAGE3_WIN_THRESHOLD = 0.50\n",
    "\n",
    "# Target food per stage (progressive difficulty)\n",
    "STAGE0_TARGET_FOOD = 10\n",
    "STAGE1_TARGET_FOOD = 10\n",
    "STAGE2_TARGET_FOOD = 4\n",
    "STAGE3_TARGET_FOOD = 6\n",
    "STAGE4_TARGET_FOOD = 8\n",
    "\n",
    "# Output\n",
    "SAVE_DIR = '../../results/weights/ppo_two_snake_mlp_curriculum'\n",
    "SEED = 42\n",
    "LOG_INTERVAL = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Environment & Agent Setup\n",
    "set_seed(SEED)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create environment\n",
    "env = VectorizedTwoSnakeEnv(\n",
    "    num_envs=NUM_ENVS,\n",
    "    grid_size=GRID_SIZE,\n",
    "    max_steps=MAX_STEPS,\n",
    "    target_food=STAGE0_TARGET_FOOD,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get input dimensions from environment\n",
    "obs1, obs2 = env.reset()\n",
    "input_dim = obs1.shape[1]  # Should be 35 for competitive features\n",
    "output_dim = 3  # relative actions\n",
    "\n",
    "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")\n",
    "\n",
    "# Create Agent 1 (Big)\n",
    "actor1 = PPO_Actor_MLP(input_dim, output_dim, BIG_HIDDEN_DIMS).to(device)\n",
    "critic1 = PPO_Critic_MLP(input_dim, BIG_HIDDEN_DIMS).to(device)\n",
    "actor1_optimizer = torch.optim.Adam(actor1.parameters(), lr=ACTOR_LR)\n",
    "critic1_optimizer = torch.optim.Adam(critic1.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "# Create Agent 2 (Small)\n",
    "actor2 = PPO_Actor_MLP(input_dim, output_dim, SMALL_HIDDEN_DIMS).to(device)\n",
    "critic2 = PPO_Critic_MLP(input_dim, SMALL_HIDDEN_DIMS).to(device)\n",
    "actor2_optimizer = torch.optim.Adam(actor2.parameters(), lr=ACTOR_LR)\n",
    "critic2_optimizer = torch.optim.Adam(critic2.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "# Load scripted opponents\n",
    "scripted_agents = {}\n",
    "for agent_type in ['static', 'random', 'greedy']:\n",
    "    try:\n",
    "        scripted_agents[agent_type] = get_scripted_agent(agent_type, device=device)\n",
    "        print(f\"Loaded {agent_type} opponent\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load {agent_type} agent: {e}\")\n",
    "\n",
    "print(f\"\\nEnvironment: {GRID_SIZE}x{GRID_SIZE} grid, {NUM_ENVS} parallel envs\")\n",
    "print(f\"Agent 1 (Big): {BIG_HIDDEN_DIMS}\")\n",
    "print(f\"Agent 2 (Small): {SMALL_HIDDEN_DIMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PPO Helper Functions\n",
    "\n",
    "class PPOBuffer:\n",
    "    \"\"\"Buffer for storing PPO rollout data\"\"\"\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "    \n",
    "    def add(self, states, actions, rewards, dones, log_probs, values):\n",
    "        self.states.append(states)\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.dones.append(dones)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.values.append(values)\n",
    "    \n",
    "    def get(self):\n",
    "        states = torch.cat(self.states, dim=0)\n",
    "        actions = torch.cat(self.actions, dim=0)\n",
    "        rewards = torch.stack(self.rewards)\n",
    "        dones = torch.stack(self.dones)\n",
    "        log_probs = torch.cat(self.log_probs, dim=0)\n",
    "        values = torch.cat(self.values, dim=0)\n",
    "        return states, actions, rewards, dones, log_probs, values\n",
    "\n",
    "def select_actions(actor, critic, states):\n",
    "    \"\"\"Select actions using current policy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = actor(states)\n",
    "        values = critic(states).squeeze(-1)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "    return actions, log_probs, values\n",
    "\n",
    "def select_greedy_actions(actor, states):\n",
    "    \"\"\"Select greedy actions (for frozen policy)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = actor(states)\n",
    "        actions = logits.argmax(dim=-1)\n",
    "    return actions\n",
    "\n",
    "def compute_gae(rewards, values, dones, next_value, gamma=GAMMA, gae_lambda=GAE_LAMBDA):\n",
    "    \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "    steps, num_envs = rewards.shape\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = torch.zeros(num_envs, device=rewards.device)\n",
    "    \n",
    "    for t in reversed(range(steps)):\n",
    "        if t == steps - 1:\n",
    "            next_val = next_value\n",
    "        else:\n",
    "            next_val = values[t + 1]\n",
    "        \n",
    "        mask = 1.0 - dones[t].float()\n",
    "        delta = rewards[t] + gamma * next_val * mask - values[t]\n",
    "        gae = delta + gamma * gae_lambda * mask * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    returns = advantages + values.view(steps, num_envs)\n",
    "    return advantages, returns\n",
    "\n",
    "def ppo_update(actor, critic, actor_optimizer, critic_optimizer, states, actions, old_log_probs, advantages, returns):\n",
    "    \"\"\"PPO update step\"\"\"\n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    total_actor_loss = 0\n",
    "    total_critic_loss = 0\n",
    "    total_entropy = 0\n",
    "    \n",
    "    # Mini-batch updates\n",
    "    num_samples = states.shape[0]\n",
    "    indices = torch.randperm(num_samples, device=states.device)\n",
    "    \n",
    "    for epoch in range(PPO_EPOCHS):\n",
    "        for start in range(0, num_samples, BATCH_SIZE):\n",
    "            end = min(start + BATCH_SIZE, num_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            batch_states = states[batch_indices]\n",
    "            batch_actions = actions[batch_indices]\n",
    "            batch_old_log_probs = old_log_probs[batch_indices]\n",
    "            batch_advantages = advantages[batch_indices]\n",
    "            batch_returns = returns[batch_indices]\n",
    "            \n",
    "            # Actor loss\n",
    "            logits = actor(batch_states)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(batch_actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "            surr1 = ratio * batch_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * batch_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEF * entropy\n",
    "            \n",
    "            # Critic loss\n",
    "            values = critic(batch_states).squeeze(-1)\n",
    "            critic_loss = F.mse_loss(values, batch_returns)\n",
    "            \n",
    "            # Update actor\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor.parameters(), MAX_GRAD_NORM)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # Update critic\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), MAX_GRAD_NORM)\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            total_actor_loss += actor_loss.item()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "            total_entropy += entropy.item()\n",
    "    \n",
    "    num_updates = PPO_EPOCHS * ((num_samples + BATCH_SIZE - 1) // BATCH_SIZE)\n",
    "    return total_actor_loss / num_updates, total_critic_loss / num_updates, total_entropy / num_updates\n",
    "\n",
    "# Global tracking\n",
    "total_steps = 0\n",
    "total_rounds = 0\n",
    "round_winners = []\n",
    "all_scores1 = []\n",
    "all_scores2 = []\n",
    "all_losses = []\n",
    "all_win_rates = []\n",
    "stage_results = []\n",
    "\n",
    "buffer1 = PPOBuffer(device)\n",
    "buffer2 = PPOBuffer(device)\n",
    "\n",
    "def calculate_win_rate(window=100):\n",
    "    if len(round_winners) < window:\n",
    "        window = len(round_winners)\n",
    "    if window == 0:\n",
    "        return 0.0\n",
    "    recent = round_winners[-window:]\n",
    "    return sum(1 for w in recent if w == 1) / window\n",
    "\n",
    "print(\"PPO helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Stage 0 - Static Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 0: Static Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE0_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE0_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE0_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage0_start = time.time()\n",
    "stage_steps = 0\n",
    "env.set_target_food(STAGE0_TARGET_FOOD)\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "while True:\n",
    "    buffer1.clear()\n",
    "    \n",
    "    # Collect rollout\n",
    "    for _ in range(max(1, ROLLOUT_STEPS // NUM_ENVS)):\n",
    "        # Agent 1 uses PPO policy\n",
    "        actions1, log_probs1, values1 = select_actions(actor1, critic1, obs1)\n",
    "        # Agent 2 uses static scripted agent\n",
    "        actions2 = scripted_agents['static'].select_action(env)\n",
    "        \n",
    "        next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "        \n",
    "        buffer1.add(obs1, actions1, r1, dones, log_probs1, values1)\n",
    "        \n",
    "        if dones.any():\n",
    "            num_done = len(info['done_envs'])\n",
    "            for i in range(num_done):\n",
    "                round_winners.append(int(info['winners'][i]))\n",
    "                total_rounds += 1\n",
    "                all_scores1.append(info['food_counts1'][i])\n",
    "                all_scores2.append(info['food_counts2'][i])\n",
    "        \n",
    "        obs1 = next_obs1\n",
    "        obs2 = next_obs2\n",
    "        stage_steps += NUM_ENVS\n",
    "        total_steps += NUM_ENVS\n",
    "    \n",
    "    # PPO update for agent 1\n",
    "    next_value1 = critic1(obs1).squeeze(-1).detach()\n",
    "    states1, actions_b1, rewards1, dones1, log_probs_b1, values_b1 = buffer1.get()\n",
    "    \n",
    "    steps_per_env = len(buffer1.states)\n",
    "    rewards1 = rewards1.view(steps_per_env, NUM_ENVS)\n",
    "    values_v1 = values_b1.view(steps_per_env, NUM_ENVS)\n",
    "    dones1 = dones1.view(steps_per_env, NUM_ENVS)\n",
    "    \n",
    "    advantages1, returns1 = compute_gae(rewards1, values_v1, dones1, next_value1)\n",
    "    advantages1 = advantages1.view(-1)\n",
    "    returns1 = returns1.view(-1)\n",
    "    \n",
    "    actor_loss, critic_loss, entropy = ppo_update(\n",
    "        actor1, critic1, actor1_optimizer, critic1_optimizer,\n",
    "        states1, actions_b1, log_probs_b1, advantages1, returns1\n",
    "    )\n",
    "    all_losses.append(actor_loss + critic_loss)\n",
    "    \n",
    "    # Logging\n",
    "    if total_steps % (LOG_INTERVAL * NUM_ENVS) < NUM_ENVS * ROLLOUT_STEPS // NUM_ENVS:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {actor_loss + critic_loss:.4f}\")\n",
    "    \n",
    "    # Check stage completion\n",
    "    if stage_steps >= STAGE0_MIN_STEPS and calculate_win_rate() >= STAGE0_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE0_MIN_STEPS * 3:  # Safety limit\n",
    "        print(\"Warning: Stage 0 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage0_time = time.time() - stage0_start\n",
    "stage0_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 0,\n",
    "    'name': 'Static',\n",
    "    'time': stage0_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage0_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 0 COMPLETE\")\n",
    "print(f\"Time: {stage0_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage0_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Stage 1 - Random Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 1: Random Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE1_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE1_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE1_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage1_start = time.time()\n",
    "stage_steps = 0\n",
    "env.set_target_food(STAGE1_TARGET_FOOD)\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "while True:\n",
    "    buffer1.clear()\n",
    "    \n",
    "    for _ in range(max(1, ROLLOUT_STEPS // NUM_ENVS)):\n",
    "        actions1, log_probs1, values1 = select_actions(actor1, critic1, obs1)\n",
    "        actions2 = scripted_agents['random'].select_action(env)\n",
    "        \n",
    "        next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "        \n",
    "        buffer1.add(obs1, actions1, r1, dones, log_probs1, values1)\n",
    "        \n",
    "        if dones.any():\n",
    "            num_done = len(info['done_envs'])\n",
    "            for i in range(num_done):\n",
    "                round_winners.append(int(info['winners'][i]))\n",
    "                total_rounds += 1\n",
    "                all_scores1.append(info['food_counts1'][i])\n",
    "                all_scores2.append(info['food_counts2'][i])\n",
    "        \n",
    "        obs1 = next_obs1\n",
    "        obs2 = next_obs2\n",
    "        stage_steps += NUM_ENVS\n",
    "        total_steps += NUM_ENVS\n",
    "    \n",
    "    # PPO update\n",
    "    next_value1 = critic1(obs1).squeeze(-1).detach()\n",
    "    states1, actions_b1, rewards1, dones1, log_probs_b1, values_b1 = buffer1.get()\n",
    "    \n",
    "    steps_per_env = len(buffer1.states)\n",
    "    rewards1 = rewards1.view(steps_per_env, NUM_ENVS)\n",
    "    values_v1 = values_b1.view(steps_per_env, NUM_ENVS)\n",
    "    dones1 = dones1.view(steps_per_env, NUM_ENVS)\n",
    "    \n",
    "    advantages1, returns1 = compute_gae(rewards1, values_v1, dones1, next_value1)\n",
    "    advantages1 = advantages1.view(-1)\n",
    "    returns1 = returns1.view(-1)\n",
    "    \n",
    "    actor_loss, critic_loss, entropy = ppo_update(\n",
    "        actor1, critic1, actor1_optimizer, critic1_optimizer,\n",
    "        states1, actions_b1, log_probs_b1, advantages1, returns1\n",
    "    )\n",
    "    all_losses.append(actor_loss + critic_loss)\n",
    "    \n",
    "    if total_steps % (LOG_INTERVAL * NUM_ENVS) < NUM_ENVS * ROLLOUT_STEPS // NUM_ENVS:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {actor_loss + critic_loss:.4f}\")\n",
    "    \n",
    "    if stage_steps >= STAGE1_MIN_STEPS and calculate_win_rate() >= STAGE1_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE1_MIN_STEPS * 3:\n",
    "        print(\"Warning: Stage 1 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage1_time = time.time() - stage1_start\n",
    "stage1_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 1,\n",
    "    'name': 'Random',\n",
    "    'time': stage1_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage1_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 1 COMPLETE\")\n",
    "print(f\"Time: {stage1_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage1_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Stage 2 - Greedy Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 2: Greedy Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE2_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE2_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE2_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage2_start = time.time()\n",
    "stage_steps = 0\n",
    "env.set_target_food(STAGE2_TARGET_FOOD)\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "while True:\n",
    "    buffer1.clear()\n",
    "    \n",
    "    for _ in range(max(1, ROLLOUT_STEPS // NUM_ENVS)):\n",
    "        actions1, log_probs1, values1 = select_actions(actor1, critic1, obs1)\n",
    "        actions2 = scripted_agents['greedy'].select_action(env)\n",
    "        \n",
    "        next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "        \n",
    "        buffer1.add(obs1, actions1, r1, dones, log_probs1, values1)\n",
    "        \n",
    "        if dones.any():\n",
    "            num_done = len(info['done_envs'])\n",
    "            for i in range(num_done):\n",
    "                round_winners.append(int(info['winners'][i]))\n",
    "                total_rounds += 1\n",
    "                all_scores1.append(info['food_counts1'][i])\n",
    "                all_scores2.append(info['food_counts2'][i])\n",
    "        \n",
    "        obs1 = next_obs1\n",
    "        obs2 = next_obs2\n",
    "        stage_steps += NUM_ENVS\n",
    "        total_steps += NUM_ENVS\n",
    "    \n",
    "    # PPO update\n",
    "    next_value1 = critic1(obs1).squeeze(-1).detach()\n",
    "    states1, actions_b1, rewards1, dones1, log_probs_b1, values_b1 = buffer1.get()\n",
    "    \n",
    "    steps_per_env = len(buffer1.states)\n",
    "    rewards1 = rewards1.view(steps_per_env, NUM_ENVS)\n",
    "    values_v1 = values_b1.view(steps_per_env, NUM_ENVS)\n",
    "    dones1 = dones1.view(steps_per_env, NUM_ENVS)\n",
    "    \n",
    "    advantages1, returns1 = compute_gae(rewards1, values_v1, dones1, next_value1)\n",
    "    advantages1 = advantages1.view(-1)\n",
    "    returns1 = returns1.view(-1)\n",
    "    \n",
    "    actor_loss, critic_loss, entropy = ppo_update(\n",
    "        actor1, critic1, actor1_optimizer, critic1_optimizer,\n",
    "        states1, actions_b1, log_probs_b1, advantages1, returns1\n",
    "    )\n",
    "    all_losses.append(actor_loss + critic_loss)\n",
    "    \n",
    "    if total_steps % (LOG_INTERVAL * NUM_ENVS) < NUM_ENVS * ROLLOUT_STEPS // NUM_ENVS:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {actor_loss + critic_loss:.4f}\")\n",
    "    \n",
    "    if stage_steps >= STAGE2_MIN_STEPS and calculate_win_rate() >= STAGE2_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE2_MIN_STEPS * 3:\n",
    "        print(\"Warning: Stage 2 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage2_time = time.time() - stage2_start\n",
    "stage2_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 2,\n",
    "    'name': 'Greedy',\n",
    "    'time': stage2_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage2_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 2 COMPLETE\")\n",
    "print(f\"Time: {stage2_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage2_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Stage 3 - Frozen Policy Opponent\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 3: Frozen Policy Opponent\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE3_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE3_MIN_STEPS}\")\n",
    "print(f\"Win rate threshold: {STAGE3_WIN_THRESHOLD}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage3_start = time.time()\n",
    "stage_steps = 0\n",
    "env.set_target_food(STAGE3_TARGET_FOOD)\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "while True:\n",
    "    buffer1.clear()\n",
    "    \n",
    "    for _ in range(max(1, ROLLOUT_STEPS // NUM_ENVS)):\n",
    "        actions1, log_probs1, values1 = select_actions(actor1, critic1, obs1)\n",
    "        # Frozen policy: use agent2's actor with greedy selection\n",
    "        actions2 = select_greedy_actions(actor2, obs2)\n",
    "        \n",
    "        next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "        \n",
    "        buffer1.add(obs1, actions1, r1, dones, log_probs1, values1)\n",
    "        \n",
    "        if dones.any():\n",
    "            num_done = len(info['done_envs'])\n",
    "            for i in range(num_done):\n",
    "                round_winners.append(int(info['winners'][i]))\n",
    "                total_rounds += 1\n",
    "                all_scores1.append(info['food_counts1'][i])\n",
    "                all_scores2.append(info['food_counts2'][i])\n",
    "        \n",
    "        obs1 = next_obs1\n",
    "        obs2 = next_obs2\n",
    "        stage_steps += NUM_ENVS\n",
    "        total_steps += NUM_ENVS\n",
    "    \n",
    "    # PPO update (only agent 1)\n",
    "    next_value1 = critic1(obs1).squeeze(-1).detach()\n",
    "    states1, actions_b1, rewards1, dones1, log_probs_b1, values_b1 = buffer1.get()\n",
    "    \n",
    "    steps_per_env = len(buffer1.states)\n",
    "    rewards1 = rewards1.view(steps_per_env, NUM_ENVS)\n",
    "    values_v1 = values_b1.view(steps_per_env, NUM_ENVS)\n",
    "    dones1 = dones1.view(steps_per_env, NUM_ENVS)\n",
    "    \n",
    "    advantages1, returns1 = compute_gae(rewards1, values_v1, dones1, next_value1)\n",
    "    advantages1 = advantages1.view(-1)\n",
    "    returns1 = returns1.view(-1)\n",
    "    \n",
    "    actor_loss, critic_loss, entropy = ppo_update(\n",
    "        actor1, critic1, actor1_optimizer, critic1_optimizer,\n",
    "        states1, actions_b1, log_probs_b1, advantages1, returns1\n",
    "    )\n",
    "    all_losses.append(actor_loss + critic_loss)\n",
    "    \n",
    "    if total_steps % (LOG_INTERVAL * NUM_ENVS) < NUM_ENVS * ROLLOUT_STEPS // NUM_ENVS:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Score: {avg_score1:.1f} | Loss: {actor_loss + critic_loss:.4f}\")\n",
    "    \n",
    "    if stage_steps >= STAGE3_MIN_STEPS and calculate_win_rate() >= STAGE3_WIN_THRESHOLD:\n",
    "        break\n",
    "    if stage_steps >= STAGE3_MIN_STEPS * 3:\n",
    "        print(\"Warning: Stage 3 reached max steps without meeting threshold\")\n",
    "        break\n",
    "\n",
    "stage3_time = time.time() - stage3_start\n",
    "stage3_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 3,\n",
    "    'name': 'Frozen',\n",
    "    'time': stage3_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage3_win_rate\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 3 COMPLETE\")\n",
    "print(f\"Time: {stage3_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage3_win_rate:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Stage 4 - Co-Evolution (Both Learning)\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 4: Co-Evolution (Both Learning)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target food: {STAGE4_TARGET_FOOD}\")\n",
    "print(f\"Min steps: {STAGE4_MIN_STEPS}\")\n",
    "print(\"Win rate threshold: None (train for full duration)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "stage4_start = time.time()\n",
    "stage_steps = 0\n",
    "env.set_target_food(STAGE4_TARGET_FOOD)\n",
    "obs1, obs2 = env.reset()\n",
    "\n",
    "while stage_steps < STAGE4_MIN_STEPS:\n",
    "    buffer1.clear()\n",
    "    buffer2.clear()\n",
    "    \n",
    "    for _ in range(max(1, ROLLOUT_STEPS // NUM_ENVS)):\n",
    "        # Both agents learning\n",
    "        actions1, log_probs1, values1 = select_actions(actor1, critic1, obs1)\n",
    "        actions2, log_probs2, values2 = select_actions(actor2, critic2, obs2)\n",
    "        \n",
    "        next_obs1, next_obs2, r1, r2, dones, info = env.step(actions1, actions2)\n",
    "        \n",
    "        buffer1.add(obs1, actions1, r1, dones, log_probs1, values1)\n",
    "        buffer2.add(obs2, actions2, r2, dones, log_probs2, values2)\n",
    "        \n",
    "        if dones.any():\n",
    "            num_done = len(info['done_envs'])\n",
    "            for i in range(num_done):\n",
    "                round_winners.append(int(info['winners'][i]))\n",
    "                total_rounds += 1\n",
    "                all_scores1.append(info['food_counts1'][i])\n",
    "                all_scores2.append(info['food_counts2'][i])\n",
    "        \n",
    "        obs1 = next_obs1\n",
    "        obs2 = next_obs2\n",
    "        stage_steps += NUM_ENVS\n",
    "        total_steps += NUM_ENVS\n",
    "        \n",
    "        if stage_steps >= STAGE4_MIN_STEPS:\n",
    "            break\n",
    "    \n",
    "    if stage_steps >= STAGE4_MIN_STEPS:\n",
    "        break\n",
    "    \n",
    "    # PPO update for both agents\n",
    "    steps_per_env = len(buffer1.states)\n",
    "    \n",
    "    # Agent 1 update\n",
    "    next_value1 = critic1(obs1).squeeze(-1).detach()\n",
    "    states1, actions_b1, rewards1, dones1, log_probs_b1, values_b1 = buffer1.get()\n",
    "    rewards1 = rewards1.view(steps_per_env, NUM_ENVS)\n",
    "    values_v1 = values_b1.view(steps_per_env, NUM_ENVS)\n",
    "    dones1 = dones1.view(steps_per_env, NUM_ENVS)\n",
    "    advantages1, returns1 = compute_gae(rewards1, values_v1, dones1, next_value1)\n",
    "    advantages1 = advantages1.view(-1)\n",
    "    returns1 = returns1.view(-1)\n",
    "    actor_loss1, critic_loss1, _ = ppo_update(\n",
    "        actor1, critic1, actor1_optimizer, critic1_optimizer,\n",
    "        states1, actions_b1, log_probs_b1, advantages1, returns1\n",
    "    )\n",
    "    \n",
    "    # Agent 2 update\n",
    "    next_value2 = critic2(obs2).squeeze(-1).detach()\n",
    "    states2, actions_b2, rewards2, dones2, log_probs_b2, values_b2 = buffer2.get()\n",
    "    rewards2 = rewards2.view(steps_per_env, NUM_ENVS)\n",
    "    values_v2 = values_b2.view(steps_per_env, NUM_ENVS)\n",
    "    dones2 = dones2.view(steps_per_env, NUM_ENVS)\n",
    "    advantages2, returns2 = compute_gae(rewards2, values_v2, dones2, next_value2)\n",
    "    advantages2 = advantages2.view(-1)\n",
    "    returns2 = returns2.view(-1)\n",
    "    actor_loss2, critic_loss2, _ = ppo_update(\n",
    "        actor2, critic2, actor2_optimizer, critic2_optimizer,\n",
    "        states2, actions_b2, log_probs_b2, advantages2, returns2\n",
    "    )\n",
    "    \n",
    "    all_losses.append(actor_loss1 + critic_loss1)\n",
    "    \n",
    "    if total_steps % (LOG_INTERVAL * NUM_ENVS) < NUM_ENVS * ROLLOUT_STEPS // NUM_ENVS:\n",
    "        win_rate = calculate_win_rate()\n",
    "        all_win_rates.append(win_rate)\n",
    "        avg_score1 = np.mean(all_scores1[-100:]) if all_scores1 else 0\n",
    "        avg_score2 = np.mean(all_scores2[-100:]) if all_scores2 else 0\n",
    "        print(f\"[Step {total_steps:>6}] Win Rate: {win_rate:.2%} | Scores: {avg_score1:.1f} vs {avg_score2:.1f}\")\n",
    "\n",
    "stage4_time = time.time() - stage4_start\n",
    "stage4_win_rate = calculate_win_rate()\n",
    "stage_results.append({\n",
    "    'stage': 4,\n",
    "    'name': 'Co-Evolution',\n",
    "    'time': stage4_time,\n",
    "    'steps': stage_steps,\n",
    "    'win_rate': stage4_win_rate\n",
    "})\n",
    "\n",
    "total_training_time = sum(r['time'] for r in stage_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STAGE 4 COMPLETE\")\n",
    "print(f\"Time: {stage4_time:.1f}s | Steps: {stage_steps:,} | Win Rate: {stage4_win_rate:.2%}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTOTAL CURRICULUM TRAINING COMPLETE!\")\n",
    "print(f\"Total time: {total_training_time:.1f}s ({total_training_time/60:.1f} min)\")\n",
    "print(f\"Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Weights\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save Agent 1 (Big)\n",
    "agent1_path = save_dir / f\"big_256x256_curriculum_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    'actor': actor1.state_dict(),\n",
    "    'critic': critic1.state_dict(),\n",
    "    'actor_optimizer': actor1_optimizer.state_dict(),\n",
    "    'critic_optimizer': critic1_optimizer.state_dict(),\n",
    "    'total_steps': total_steps,\n",
    "    'config': {\n",
    "        'hidden_dims': BIG_HIDDEN_DIMS,\n",
    "        'grid_size': GRID_SIZE,\n",
    "        'actor_lr': ACTOR_LR,\n",
    "        'critic_lr': CRITIC_LR\n",
    "    }\n",
    "}, agent1_path)\n",
    "print(f\"Agent 1 (Big) saved to: {agent1_path}\")\n",
    "\n",
    "# Save Agent 2 (Small)\n",
    "agent2_path = save_dir / f\"small_128x128_curriculum_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    'actor': actor2.state_dict(),\n",
    "    'critic': critic2.state_dict(),\n",
    "    'actor_optimizer': actor2_optimizer.state_dict(),\n",
    "    'critic_optimizer': critic2_optimizer.state_dict(),\n",
    "    'total_steps': total_steps,\n",
    "    'config': {\n",
    "        'hidden_dims': SMALL_HIDDEN_DIMS,\n",
    "        'grid_size': GRID_SIZE,\n",
    "        'actor_lr': ACTOR_LR,\n",
    "        'critic_lr': CRITIC_LR\n",
    "    }\n",
    "}, agent2_path)\n",
    "print(f\"Agent 2 (Small) saved to: {agent2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "def smooth(data, window=50):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot 1: Agent Scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(all_scores1, alpha=0.3, label='Agent 1 (Big)', color='blue')\n",
    "ax1.plot(all_scores2, alpha=0.3, label='Agent 2 (Small)', color='red')\n",
    "if len(all_scores1) > 50:\n",
    "    ax1.plot(range(49, len(all_scores1)), smooth(all_scores1), color='blue', linewidth=2)\n",
    "    ax1.plot(range(49, len(all_scores2)), smooth(all_scores2), color='red', linewidth=2)\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Score (Food Eaten)')\n",
    "ax1.set_title('Agent Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Win Rate Over Time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(all_win_rates, color='green')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', label='50% (balanced)')\n",
    "ax2.set_xlabel('Training Progress')\n",
    "ax2.set_ylabel('Agent 1 Win Rate')\n",
    "ax2.set_title('Win Rate Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Loss\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(all_losses, alpha=0.5)\n",
    "if len(all_losses) > 50:\n",
    "    ax3.plot(range(49, len(all_losses)), smooth(all_losses), color='blue', linewidth=2)\n",
    "ax3.set_xlabel('Update')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('PPO Loss')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Stage Results\n",
    "ax4 = axes[1, 1]\n",
    "stage_names = [r['name'] for r in stage_results]\n",
    "stage_win_rates = [r['win_rate'] * 100 for r in stage_results]\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "bars = ax4.bar(stage_names, stage_win_rates, color=colors[:len(stage_results)])\n",
    "ax4.set_xlabel('Stage')\n",
    "ax4.set_ylabel('Final Win Rate (%)')\n",
    "ax4.set_title('Win Rate by Curriculum Stage')\n",
    "ax4.set_ylim(0, 100)\n",
    "for bar, rate in zip(bars, stage_win_rates):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{rate:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'PPO Two-Snake Curriculum Training Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = project_root / 'results' / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig_path = fig_dir / f'ppo_two_snake_curriculum_{timestamp}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Results Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"CURRICULUM TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Algorithm: PPO Two-Snake Curriculum (MLP)\")\n",
    "print(f\"Grid Size: {GRID_SIZE}x{GRID_SIZE}\")\n",
    "print(f\"Total Steps: {total_steps:,}\")\n",
    "print(f\"Total Rounds: {total_rounds:,}\")\n",
    "print(f\"Total Training Time: {total_training_time:.1f}s ({total_training_time/60:.1f} min)\")\n",
    "print()\n",
    "print(\"Stage Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Stage':<15} {'Opponent':<15} {'Steps':>10} {'Time':>10} {'Win Rate':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for r in stage_results:\n",
    "    print(f\"Stage {r['stage']:<8} {r['name']:<15} {r['steps']:>10,} {r['time']:>9.1f}s {r['win_rate']:>11.2%}\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Final Performance (last 100 rounds):\")\n",
    "print(f\"  Agent 1 Avg Score: {np.mean(all_scores1[-100:]):.2f}\")\n",
    "print(f\"  Agent 2 Avg Score: {np.mean(all_scores2[-100:]):.2f}\")\n",
    "print(f\"  Final Win Rate: {calculate_win_rate():.2%}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
