{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling DQN Training Notebook\n",
    "\n",
    "This notebook trains a Dueling DQN agent to play Snake.\n",
    "\n",
    "**Algorithm**: Dueling DQN with:\n",
    "- Separate value and advantage streams\n",
    "- V(s) + A(s,a) - mean(A(s,a)) architecture\n",
    "- Better value estimation for states where actions don't matter\n",
    "- Experience replay + Target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from core.environment_vectorized import VectorizedSnakeEnv\n",
    "from core.networks import DuelingDQN_MLP, DQN_CNN\n",
    "from core.utils import ReplayBuffer, EpsilonScheduler, MetricsTracker, set_seed, get_device\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (papermill parameters)\n",
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Environment\n",
    "GRID_SIZE = 10\n",
    "NUM_ENVS = 256\n",
    "MAX_STEPS = 1000\n",
    "ACTION_SPACE_TYPE = 'relative'\n",
    "STATE_REPRESENTATION = 'feature'  # 'feature' or 'grid'\n",
    "USE_FLOOD_FILL = False\n",
    "USE_ENHANCED_FEATURES = False\n",
    "USE_SELECTIVE_FEATURES = False\n",
    "\n",
    "# Training\n",
    "NUM_EPISODES = 500\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100_000\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "MIN_BUFFER_SIZE = 1000\n",
    "TRAIN_STEPS_RATIO = 0.03125\n",
    "\n",
    "# Exploration\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "\n",
    "# Network\n",
    "HIDDEN_DIMS = (128, 128)\n",
    "\n",
    "# Dueling DQN - ENABLED\n",
    "USE_DUELING = True\n",
    "\n",
    "# Output\n",
    "SAVE_DIR = '../../results/weights/dueling_dqn'\n",
    "SEED = 42\n",
    "LOG_INTERVAL = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Environment & Model Setup\n",
    "set_seed(SEED)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Dueling Architecture: {USE_DUELING}\")\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = VectorizedSnakeEnv(\n",
    "    num_envs=NUM_ENVS,\n",
    "    grid_size=GRID_SIZE,\n",
    "    action_space_type=ACTION_SPACE_TYPE,\n",
    "    state_representation=STATE_REPRESENTATION,\n",
    "    max_steps=MAX_STEPS,\n",
    "    use_flood_fill=USE_FLOOD_FILL,\n",
    "    use_enhanced_features=USE_ENHANCED_FEATURES,\n",
    "    use_selective_features=USE_SELECTIVE_FEATURES,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Determine input dimension\n",
    "input_dim = 11\n",
    "if USE_FLOOD_FILL:\n",
    "    input_dim = 14\n",
    "if USE_SELECTIVE_FEATURES:\n",
    "    input_dim = 19\n",
    "if USE_ENHANCED_FEATURES:\n",
    "    input_dim = 24\n",
    "\n",
    "if STATE_REPRESENTATION == 'feature':\n",
    "    policy_net = DuelingDQN_MLP(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=env.action_space.n,\n",
    "        hidden_dims=HIDDEN_DIMS\n",
    "    ).to(device)\n",
    "    \n",
    "    target_net = DuelingDQN_MLP(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=env.action_space.n,\n",
    "        hidden_dims=HIDDEN_DIMS\n",
    "    ).to(device)\n",
    "else:\n",
    "    # Fall back to CNN for grid representation\n",
    "    policy_net = DQN_CNN(\n",
    "        grid_size=GRID_SIZE,\n",
    "        input_channels=3,\n",
    "        output_dim=env.action_space.n\n",
    "    ).to(device)\n",
    "    \n",
    "    target_net = DQN_CNN(\n",
    "        grid_size=GRID_SIZE,\n",
    "        input_channels=3,\n",
    "        output_dim=env.action_space.n\n",
    "    ).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(capacity=BUFFER_SIZE, seed=SEED)\n",
    "epsilon_scheduler = EpsilonScheduler(\n",
    "    epsilon_start=EPSILON_START,\n",
    "    epsilon_end=EPSILON_END,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    decay_type='exponential'\n",
    ")\n",
    "metrics = MetricsTracker(window_size=100)\n",
    "\n",
    "print(f\"Environment: {GRID_SIZE}x{GRID_SIZE} grid, {NUM_ENVS} parallel envs\")\n",
    "print(f\"Policy network: {sum(p.numel() for p in policy_net.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Training Loop\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def select_actions(states, epsilon):\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(states)\n",
    "        greedy_actions = q_values.argmax(dim=1)\n",
    "    \n",
    "    random_mask = torch.rand(NUM_ENVS, device=device) < epsilon\n",
    "    random_actions = torch.randint(0, env.action_space.n, (NUM_ENVS,), device=device)\n",
    "    return torch.where(random_mask, random_actions, greedy_actions)\n",
    "\n",
    "def train_step():\n",
    "    if not replay_buffer.is_ready(BATCH_SIZE):\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "    states = states.to(device)\n",
    "    actions = actions.to(device)\n",
    "    rewards = rewards.to(device)\n",
    "    next_states = next_states.to(device)\n",
    "    dones = dones.to(device)\n",
    "    \n",
    "    current_q_values = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states).max(1)[0]\n",
    "        target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "    \n",
    "    loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"Starting Dueling DQN Training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print()\n",
    "\n",
    "states = env.reset(seed=SEED)\n",
    "start_time = time.time()\n",
    "\n",
    "episode_rewards = torch.zeros(NUM_ENVS, device=device)\n",
    "episode_lengths = torch.zeros(NUM_ENVS, dtype=torch.long, device=device)\n",
    "\n",
    "episode = 0\n",
    "total_steps = 0\n",
    "\n",
    "all_rewards = []\n",
    "all_scores = []\n",
    "all_losses = []\n",
    "all_epsilons = []\n",
    "\n",
    "while episode < NUM_EPISODES:\n",
    "    epsilon = epsilon_scheduler.get_epsilon()\n",
    "    actions = select_actions(states, epsilon)\n",
    "    next_states, rewards, dones, info = env.step(actions)\n",
    "    \n",
    "    episode_rewards += rewards\n",
    "    episode_lengths += 1\n",
    "    \n",
    "    for i in range(NUM_ENVS):\n",
    "        replay_buffer.push(\n",
    "            states[i].cpu().numpy(),\n",
    "            actions[i].item(),\n",
    "            rewards[i].item(),\n",
    "            next_states[i].cpu().numpy(),\n",
    "            dones[i].item()\n",
    "        )\n",
    "    \n",
    "    if replay_buffer.is_ready(MIN_BUFFER_SIZE):\n",
    "        num_train_steps = max(1, int(NUM_ENVS * TRAIN_STEPS_RATIO))\n",
    "        for _ in range(num_train_steps):\n",
    "            loss = train_step()\n",
    "            if loss is not None:\n",
    "                metrics.add_loss(loss)\n",
    "                all_losses.append(loss)\n",
    "    \n",
    "    if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if dones.any():\n",
    "        done_indices = torch.where(dones)[0]\n",
    "        for idx in done_indices:\n",
    "            reward = episode_rewards[idx].item()\n",
    "            length = episode_lengths[idx].item()\n",
    "            score = info['scores'][idx].item()\n",
    "            \n",
    "            metrics.add_episode(reward, length, score)\n",
    "            all_rewards.append(reward)\n",
    "            all_scores.append(score)\n",
    "            all_epsilons.append(epsilon)\n",
    "            \n",
    "            episode += 1\n",
    "            episode_rewards[idx] = 0\n",
    "            episode_lengths[idx] = 0\n",
    "            epsilon_scheduler.step()\n",
    "            \n",
    "            if episode % LOG_INTERVAL == 0:\n",
    "                stats = metrics.get_recent_stats()\n",
    "                elapsed = time.time() - start_time\n",
    "                fps = total_steps / elapsed if elapsed > 0 else 0\n",
    "                print(f\"Episode {episode}/{NUM_EPISODES} | \"\n",
    "                      f\"Score: {stats['avg_score']:.2f} | \"\n",
    "                      f\"Reward: {stats['avg_reward']:.2f} | \"\n",
    "                      f\"Epsilon: {epsilon:.4f} | \"\n",
    "                      f\"FPS: {fps:.0f}\")\n",
    "            \n",
    "            if episode >= NUM_EPISODES:\n",
    "                break\n",
    "    \n",
    "    states = next_states\n",
    "    total_steps += NUM_ENVS\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Total time: {training_time:.1f}s\")\n",
    "print(f\"Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save Weights\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"dueling_dqn_{GRID_SIZE}x{GRID_SIZE}_{NUM_EPISODES}ep_{timestamp}.pt\"\n",
    "filepath = save_dir / filename\n",
    "\n",
    "torch.save({\n",
    "    'policy_net': policy_net.state_dict(),\n",
    "    'target_net': target_net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'episode': episode,\n",
    "    'total_steps': total_steps,\n",
    "    'epsilon': epsilon_scheduler.get_epsilon(),\n",
    "    'config': {\n",
    "        'grid_size': GRID_SIZE,\n",
    "        'num_envs': NUM_ENVS,\n",
    "        'hidden_dims': HIDDEN_DIMS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'gamma': GAMMA,\n",
    "        'use_dueling': USE_DUELING,\n",
    "        'state_representation': STATE_REPRESENTATION\n",
    "    }\n",
    "}, filepath)\n",
    "\n",
    "print(f\"Model saved to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "def smooth(data, window=50):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(all_rewards, alpha=0.3, label='Raw')\n",
    "if len(all_rewards) > 50:\n",
    "    ax1.plot(range(49, len(all_rewards)), smooth(all_rewards), label='Smoothed (50)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Rewards')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(all_scores, alpha=0.3, label='Raw')\n",
    "if len(all_scores) > 50:\n",
    "    ax2.plot(range(49, len(all_scores)), smooth(all_scores), label='Smoothed (50)')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Score (Food Eaten)')\n",
    "ax2.set_title('Episode Scores')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "if all_losses:\n",
    "    step = max(1, len(all_losses) // 1000)\n",
    "    ax3.plot(all_losses[::step], alpha=0.5)\n",
    "    ax3.set_xlabel('Training Step (subsampled)')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.set_title('Training Loss')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(all_epsilons)\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Epsilon')\n",
    "ax4.set_title('Exploration Rate (Epsilon)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Dueling DQN Training Results - {NUM_EPISODES} Episodes', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = project_root / 'results' / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig_path = fig_dir / f'dueling_dqn_training_{timestamp}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Results Summary\n",
    "stats = metrics.get_recent_stats()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Algorithm: Dueling DQN\")\n",
    "print(f\"Episodes: {episode}\")\n",
    "print(f\"Total Steps: {total_steps:,}\")\n",
    "print(f\"Training Time: {training_time:.1f}s\")\n",
    "print()\n",
    "print(\"Final Performance (last 100 episodes):\")\n",
    "print(f\"  Average Score: {stats['avg_score']:.2f}\")\n",
    "print(f\"  Average Reward: {stats['avg_reward']:.2f}\")\n",
    "print(f\"  Average Length: {stats['avg_length']:.2f}\")\n",
    "print(f\"  Max Score: {stats['max_score']}\")\n",
    "print()\n",
    "print(f\"Overall Statistics:\")\n",
    "print(f\"  Mean Score: {np.mean(all_scores):.2f} +/- {np.std(all_scores):.2f}\")\n",
    "print(f\"  Max Score: {max(all_scores)}\")\n",
    "print(f\"  Mean Reward: {np.mean(all_rewards):.2f} +/- {np.std(all_rewards):.2f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
