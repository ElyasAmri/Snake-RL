{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "This notebook evaluates all trained RL models and produces comparison figures for the final report.\n",
    "\n",
    "**Models Evaluated:**\n",
    "- Single-Snake: DQN, Double DQN, Dueling DQN, PER DQN, Noisy DQN, PPO, A2C, REINFORCE\n",
    "- Two-Snake: Classic DQN, PPO Co-evolution, PPO Curriculum, DQN Curriculum\n",
    "- Baselines: Random Agent, Shortest Path (A*)\n",
    "\n",
    "**Output:**\n",
    "- Comparison figures saved to `results/figures/`\n",
    "- Metrics saved to `results/data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Imports\nimport sys\nfrom pathlib import Path\n\n# Add project root to path (handle both notebook and papermill execution)\nif Path.cwd().name == 'notebooks':\n    project_root = Path.cwd().parent\nelse:\n    # When run with papermill, cwd might be project root\n    project_root = Path.cwd()\n    if not (project_root / 'core').exists():\n        project_root = project_root.parent\n\nsys.path.insert(0, str(project_root))\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple, Optional\n\nfrom core.environment_vectorized import VectorizedSnakeEnv\nfrom core.environment_two_snake_vectorized import VectorizedTwoSnakeEnv\nfrom core.environment import SnakeEnv\nfrom core.networks import (\n    DQN_MLP, DQN_CNN, \n    DuelingDQN_MLP, NoisyDQN_MLP,\n    PPO_Actor_MLP, PPO_Critic_MLP,\n    PPO_Actor_CNN, PPO_Critic_CNN\n)\nfrom core.utils import set_seed, get_device\nfrom scripts.baselines.random_agent import RandomAgent\nfrom scripts.baselines.shortest_path import ShortestPathAgent\n\nprint(f\"Project root: {project_root}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Evaluation settings\n",
    "NUM_EPISODES = 100          # Episodes per model\n",
    "GRID_SIZE = 10              # Standard grid\n",
    "MAX_STEPS = 1000            # Max steps per episode\n",
    "SEED = 42                   # Reproducibility\n",
    "NUM_ENVS = 100              # Parallel environments for speed\n",
    "\n",
    "# Paths\n",
    "WEIGHTS_DIR = project_root / 'results' / 'weights'\n",
    "FIGURES_DIR = project_root / 'results' / 'figures'\n",
    "DATA_DIR = project_root / 'results' / 'data'\n",
    "\n",
    "# Ensure directories exist\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "set_seed(SEED)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Weights directory: {WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Model Loading Functions\n\ndef get_input_dim(use_flood_fill: bool = False, use_selective: bool = False, use_enhanced: bool = False) -> int:\n    \"\"\"Get input dimension based on feature flags\"\"\"\n    if use_enhanced:\n        return 24\n    if use_selective:\n        return 19\n    if use_flood_fill:\n        return 14\n    return 11\n\n\ndef parse_model_info(filename: str) -> Dict:\n    \"\"\"Parse model information from filename\"\"\"\n    info = {\n        'algorithm': 'unknown',\n        'network': 'mlp',\n        'state_rep': 'basic',\n        'use_flood_fill': False,\n        'use_enhanced': False,\n        'use_selective': False,\n        'hidden_dims': (128, 128)\n    }\n    \n    filename_lower = filename.lower()\n    \n    # Detect algorithm\n    if 'dueling' in filename_lower:\n        info['algorithm'] = 'dueling_dqn'\n    elif 'noisy' in filename_lower:\n        info['algorithm'] = 'noisy_dqn'\n    elif 'per_dqn' in filename_lower:\n        info['algorithm'] = 'per_dqn'\n    elif 'double_dqn' in filename_lower:\n        info['algorithm'] = 'double_dqn'\n    elif 'dqn' in filename_lower:\n        info['algorithm'] = 'dqn'\n    elif 'ppo' in filename_lower:\n        info['algorithm'] = 'ppo'\n    elif 'a2c' in filename_lower:\n        info['algorithm'] = 'a2c'\n    elif 'reinforce' in filename_lower:\n        info['algorithm'] = 'reinforce'\n    \n    # Detect network type\n    if 'cnn' in filename_lower:\n        info['network'] = 'cnn'\n    \n    # Detect state representation\n    if 'floodfill' in filename_lower or 'flood_fill' in filename_lower:\n        info['use_flood_fill'] = True\n        info['state_rep'] = 'flood_fill'\n    if 'enhanced' in filename_lower:\n        info['use_enhanced'] = True\n        info['state_rep'] = 'enhanced'\n    if 'selective' in filename_lower:\n        info['use_selective'] = True\n        info['state_rep'] = 'selective'\n    \n    # Detect hidden dims\n    if '256x256' in filename_lower:\n        info['hidden_dims'] = (256, 256)\n    \n    return info\n\n\ndef load_dqn_model(filepath: Path, device: torch.device) -> Tuple[torch.nn.Module, Dict]:\n    \"\"\"Load a DQN-family model from checkpoint\"\"\"\n    checkpoint = torch.load(filepath, map_location=device, weights_only=False)\n    info = parse_model_info(filepath.name)\n    \n    # Get input dimension\n    input_dim = get_input_dim(\n        use_flood_fill=info['use_flood_fill'],\n        use_selective=info['use_selective'],\n        use_enhanced=info['use_enhanced']\n    )\n    \n    # Create model based on type\n    if info['network'] == 'cnn':\n        model = DQN_CNN(grid_size=GRID_SIZE, input_channels=3, output_dim=3)\n    elif info['algorithm'] == 'dueling_dqn':\n        model = DuelingDQN_MLP(input_dim=input_dim, output_dim=3, hidden_dims=info['hidden_dims'])\n    elif info['algorithm'] == 'noisy_dqn':\n        model = NoisyDQN_MLP(input_dim=input_dim, output_dim=3, hidden_dims=info['hidden_dims'])\n    else:\n        model = DQN_MLP(input_dim=input_dim, output_dim=3, hidden_dims=info['hidden_dims'])\n    \n    # Load weights\n    if 'policy_net' in checkpoint:\n        model.load_state_dict(checkpoint['policy_net'])\n    elif 'model' in checkpoint:\n        model.load_state_dict(checkpoint['model'])\n    else:\n        model.load_state_dict(checkpoint)\n    \n    model.to(device)\n    model.eval()\n    \n    return model, info\n\n\ndef load_ppo_model(filepath: Path, device: torch.device) -> Tuple[torch.nn.Module, Dict]:\n    \"\"\"Load a PPO/A2C/REINFORCE model from checkpoint\"\"\"\n    checkpoint = torch.load(filepath, map_location=device, weights_only=False)\n    info = parse_model_info(filepath.name)\n    \n    # Get input dimension\n    input_dim = get_input_dim(\n        use_flood_fill=info['use_flood_fill'],\n        use_selective=info['use_selective'],\n        use_enhanced=info['use_enhanced']\n    )\n    \n    # Create model based on type\n    if info['network'] == 'cnn':\n        model = PPO_Actor_CNN(grid_size=GRID_SIZE, input_channels=3, output_dim=3)\n    else:\n        model = PPO_Actor_MLP(input_dim=input_dim, output_dim=3, hidden_dims=info['hidden_dims'])\n    \n    # Load weights - handle different checkpoint formats\n    if 'actor' in checkpoint:\n        model.load_state_dict(checkpoint['actor'])\n    elif 'actor_state_dict' in checkpoint:\n        # A2C format\n        model.load_state_dict(checkpoint['actor_state_dict'])\n    elif 'policy' in checkpoint:\n        # REINFORCE format\n        model.load_state_dict(checkpoint['policy'])\n    elif 'policy_net' in checkpoint:\n        model.load_state_dict(checkpoint['policy_net'])\n    elif 'model' in checkpoint:\n        model.load_state_dict(checkpoint['model'])\n    else:\n        model.load_state_dict(checkpoint)\n    \n    model.to(device)\n    model.eval()\n    \n    return model, info\n\n\nprint(\"Model loading functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluation Functions\n",
    "\n",
    "def evaluate_dqn_model(\n",
    "    model: torch.nn.Module,\n",
    "    info: Dict,\n",
    "    num_episodes: int = NUM_EPISODES,\n",
    "    device: torch.device = device\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate a DQN-family model\"\"\"\n",
    "    \n",
    "    # Create environment with matching state representation\n",
    "    state_rep = 'grid' if info['network'] == 'cnn' else 'feature'\n",
    "    \n",
    "    env = VectorizedSnakeEnv(\n",
    "        num_envs=num_episodes,\n",
    "        grid_size=GRID_SIZE,\n",
    "        action_space_type='relative',\n",
    "        state_representation=state_rep,\n",
    "        max_steps=MAX_STEPS,\n",
    "        use_flood_fill=info['use_flood_fill'],\n",
    "        use_enhanced_features=info['use_enhanced'],\n",
    "        use_selective_features=info['use_selective'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    obs = env.reset(seed=SEED)\n",
    "    \n",
    "    scores = np.zeros(num_episodes)\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    lengths = np.zeros(num_episodes)\n",
    "    done_mask = np.zeros(num_episodes, dtype=bool)\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        with torch.no_grad():\n",
    "            q_values = model(obs)\n",
    "            actions = q_values.argmax(dim=1)\n",
    "        \n",
    "        obs, step_rewards, dones, info_dict = env.step(actions)\n",
    "        \n",
    "        # Track rewards for non-done episodes\n",
    "        episode_rewards += step_rewards.cpu().numpy() * ~done_mask\n",
    "        \n",
    "        # Record finished episodes\n",
    "        new_done = dones.cpu().numpy() & ~done_mask\n",
    "        if new_done.any():\n",
    "            done_indices = np.where(new_done)[0]\n",
    "            for idx in done_indices:\n",
    "                scores[idx] = info_dict['scores'][idx].item()\n",
    "                rewards[idx] = episode_rewards[idx]\n",
    "                lengths[idx] = step + 1\n",
    "            done_mask |= new_done\n",
    "        \n",
    "        if done_mask.all():\n",
    "            break\n",
    "    \n",
    "    # Handle any remaining episodes\n",
    "    remaining = ~done_mask\n",
    "    if remaining.any():\n",
    "        scores[remaining] = info_dict['scores'][remaining].cpu().numpy()\n",
    "        rewards[remaining] = episode_rewards[remaining]\n",
    "        lengths[remaining] = MAX_STEPS\n",
    "    \n",
    "    return {\n",
    "        'avg_score': float(np.mean(scores)),\n",
    "        'std_score': float(np.std(scores)),\n",
    "        'max_score': int(np.max(scores)),\n",
    "        'min_score': int(np.min(scores)),\n",
    "        'avg_reward': float(np.mean(rewards)),\n",
    "        'avg_length': float(np.mean(lengths)),\n",
    "        'scores': scores.tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_policy_model(\n",
    "    model: torch.nn.Module,\n",
    "    info: Dict,\n",
    "    num_episodes: int = NUM_EPISODES,\n",
    "    device: torch.device = device\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate a policy gradient model (PPO, A2C, REINFORCE)\"\"\"\n",
    "    \n",
    "    # Create environment with matching state representation\n",
    "    state_rep = 'grid' if info['network'] == 'cnn' else 'feature'\n",
    "    \n",
    "    env = VectorizedSnakeEnv(\n",
    "        num_envs=num_episodes,\n",
    "        grid_size=GRID_SIZE,\n",
    "        action_space_type='relative',\n",
    "        state_representation=state_rep,\n",
    "        max_steps=MAX_STEPS,\n",
    "        use_flood_fill=info['use_flood_fill'],\n",
    "        use_enhanced_features=info['use_enhanced'],\n",
    "        use_selective_features=info['use_selective'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    obs = env.reset(seed=SEED)\n",
    "    \n",
    "    scores = np.zeros(num_episodes)\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    lengths = np.zeros(num_episodes)\n",
    "    done_mask = np.zeros(num_episodes, dtype=bool)\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        with torch.no_grad():\n",
    "            logits = model(obs)\n",
    "            # Use greedy action selection for evaluation\n",
    "            actions = logits.argmax(dim=1)\n",
    "        \n",
    "        obs, step_rewards, dones, info_dict = env.step(actions)\n",
    "        \n",
    "        # Track rewards for non-done episodes\n",
    "        episode_rewards += step_rewards.cpu().numpy() * ~done_mask\n",
    "        \n",
    "        # Record finished episodes\n",
    "        new_done = dones.cpu().numpy() & ~done_mask\n",
    "        if new_done.any():\n",
    "            done_indices = np.where(new_done)[0]\n",
    "            for idx in done_indices:\n",
    "                scores[idx] = info_dict['scores'][idx].item()\n",
    "                rewards[idx] = episode_rewards[idx]\n",
    "                lengths[idx] = step + 1\n",
    "            done_mask |= new_done\n",
    "        \n",
    "        if done_mask.all():\n",
    "            break\n",
    "    \n",
    "    # Handle any remaining episodes\n",
    "    remaining = ~done_mask\n",
    "    if remaining.any():\n",
    "        scores[remaining] = info_dict['scores'][remaining].cpu().numpy()\n",
    "        rewards[remaining] = episode_rewards[remaining]\n",
    "        lengths[remaining] = MAX_STEPS\n",
    "    \n",
    "    return {\n",
    "        'avg_score': float(np.mean(scores)),\n",
    "        'std_score': float(np.std(scores)),\n",
    "        'max_score': int(np.max(scores)),\n",
    "        'min_score': int(np.min(scores)),\n",
    "        'avg_reward': float(np.mean(rewards)),\n",
    "        'avg_length': float(np.mean(lengths)),\n",
    "        'scores': scores.tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_baseline(\n",
    "    agent,\n",
    "    num_episodes: int = NUM_EPISODES\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate a baseline agent (Random or A*)\"\"\"\n",
    "    \n",
    "    env = SnakeEnv(\n",
    "        grid_size=GRID_SIZE,\n",
    "        action_space_type='relative',\n",
    "        state_representation='feature',\n",
    "        max_steps=MAX_STEPS\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset(seed=SEED + episode)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(MAX_STEPS):\n",
    "            action = agent.get_action(env)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        scores.append(info['score'])\n",
    "        rewards.append(total_reward)\n",
    "        lengths.append(step + 1)\n",
    "    \n",
    "    return {\n",
    "        'avg_score': float(np.mean(scores)),\n",
    "        'std_score': float(np.std(scores)),\n",
    "        'max_score': int(np.max(scores)),\n",
    "        'min_score': int(np.min(scores)),\n",
    "        'avg_reward': float(np.mean(rewards)),\n",
    "        'avg_length': float(np.mean(lengths)),\n",
    "        'scores': scores\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Single-Snake Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Discover and Evaluate Single-Snake Models\n",
    "\n",
    "# Define which weight files to evaluate (prefer 5000ep versions when available)\n",
    "single_snake_models = {\n",
    "    # DQN variants\n",
    "    'DQN MLP Basic': 'dqn_mlp_128x128_5000ep_*.pt',\n",
    "    'DQN MLP Flood-fill': 'dqn_mlp_floodfill_128x128_5000ep_*.pt',\n",
    "    'DQN CNN': 'dqn_cnn_5000ep_*.pt',\n",
    "    'Double DQN MLP': 'double_dqn_mlp_128x128_5000ep_*.pt',\n",
    "    'Double DQN MLP Flood-fill': 'double_dqn_mlp_floodfill_128x128_5000ep_*.pt',\n",
    "    'Double DQN CNN': 'double_dqn_cnn_5000ep_*.pt',\n",
    "    'Dueling DQN MLP': 'dueling_dqn_mlp_128x128_5000ep_*.pt',\n",
    "    'Dueling DQN MLP Flood-fill': 'dueling_dqn_mlp_floodfill_128x128_5000ep_*.pt',\n",
    "    'PER DQN MLP': 'per_dqn_mlp_128x128_5000ep_*.pt',\n",
    "    'PER DQN MLP Flood-fill': 'per_dqn_mlp_floodfill_128x128_5000ep_*.pt',\n",
    "    # Policy gradient\n",
    "    'PPO MLP': 'ppo_mlp_128x128_5000ep_*.pt',\n",
    "    'PPO MLP Flood-fill': 'ppo_mlp_floodfill_128x128_5000ep_*.pt',\n",
    "    'PPO CNN': 'ppo_cnn_5000ep_*.pt',\n",
    "    'A2C MLP Flood-fill': 'a2c_floodfill_128x128_5000ep_*.pt',\n",
    "    'REINFORCE MLP': 'reinforce_mlp_128x128_5000ep_*.pt',\n",
    "    'REINFORCE MLP Flood-fill': 'reinforce_mlp_floodfill_128x128_5000ep_*.pt',\n",
    "    'REINFORCE CNN': 'reinforce_cnn_5000ep_*.pt',\n",
    "}\n",
    "\n",
    "# Find weight files\n",
    "def find_weight_file(pattern: str) -> Optional[Path]:\n",
    "    \"\"\"Find weight file matching pattern\"\"\"\n",
    "    matches = list(WEIGHTS_DIR.glob(pattern))\n",
    "    if matches:\n",
    "        # Return most recent if multiple\n",
    "        return sorted(matches, key=lambda x: x.stat().st_mtime, reverse=True)[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "print(\"Evaluating single-snake models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, pattern in single_snake_models.items():\n",
    "    filepath = find_weight_file(pattern)\n",
    "    \n",
    "    if filepath is None:\n",
    "        print(f\"  {model_name}: NOT FOUND ({pattern})\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  {model_name}: {filepath.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Determine if DQN or policy gradient\n",
    "        is_policy = any(x in model_name.lower() for x in ['ppo', 'a2c', 'reinforce'])\n",
    "        \n",
    "        if is_policy:\n",
    "            model, info = load_ppo_model(filepath, device)\n",
    "            result = evaluate_policy_model(model, info)\n",
    "        else:\n",
    "            model, info = load_dqn_model(filepath, device)\n",
    "            result = evaluate_dqn_model(model, info)\n",
    "        \n",
    "        result['model_path'] = str(filepath)\n",
    "        result['algorithm'] = info['algorithm']\n",
    "        result['network'] = info['network']\n",
    "        result['state_rep'] = info['state_rep']\n",
    "        \n",
    "        results[model_name] = result\n",
    "        print(f\"    -> Avg Score: {result['avg_score']:.2f} +/- {result['std_score']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    -> ERROR: {e}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n",
    "print(f\"Successfully evaluated: {len(results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate Baselines\n",
    "\n",
    "print(\"Evaluating baseline agents...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Random Agent\n",
    "print(\"  Random Agent...\")\n",
    "random_agent = RandomAgent(action_space_type='relative', seed=SEED)\n",
    "random_result = evaluate_baseline(random_agent)\n",
    "random_result['algorithm'] = 'baseline'\n",
    "random_result['network'] = 'n/a'\n",
    "random_result['state_rep'] = 'n/a'\n",
    "results['Random Agent'] = random_result\n",
    "print(f\"    -> Avg Score: {random_result['avg_score']:.2f} +/- {random_result['std_score']:.2f}\")\n",
    "\n",
    "# Shortest Path (A*)\n",
    "print(\"  Shortest Path (A*)...\")\n",
    "astar_agent = ShortestPathAgent(action_space_type='relative')\n",
    "astar_result = evaluate_baseline(astar_agent)\n",
    "astar_result['algorithm'] = 'baseline'\n",
    "astar_result['network'] = 'n/a'\n",
    "astar_result['state_rep'] = 'n/a'\n",
    "results['Shortest Path (A*)'] = astar_result\n",
    "print(f\"    -> Avg Score: {astar_result['avg_score']:.2f} +/- {astar_result['std_score']:.2f}\")\n",
    "\n",
    "print(\"\\nBaseline evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Create Results DataFrame\n",
    "\n",
    "# Build summary dataframe\n",
    "summary_data = []\n",
    "for model_name, result in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Avg Score': result['avg_score'],\n",
    "        'Std Score': result['std_score'],\n",
    "        'Max Score': result['max_score'],\n",
    "        'Min Score': result['min_score'],\n",
    "        'Avg Reward': result['avg_reward'],\n",
    "        'Avg Length': result['avg_length'],\n",
    "        'Algorithm': result.get('algorithm', 'unknown'),\n",
    "        'Network': result.get('network', 'unknown'),\n",
    "        'State Rep': result.get('state_rep', 'unknown')\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df = df.sort_values('Avg Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SINGLE-SNAKE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(df[['Model', 'Avg Score', 'Std Score', 'Max Score', 'Avg Length']].to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Generate Comparison Charts\n",
    "\n",
    "# Sort by average score for plotting\n",
    "df_sorted = df.sort_values('Avg Score', ascending=True)\n",
    "\n",
    "# Figure 1: Overall Comparison Bar Chart\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = []\n",
    "for model in df_sorted['Model']:\n",
    "    if 'Random' in model or 'Shortest' in model:\n",
    "        colors.append('#888888')  # Gray for baselines\n",
    "    elif 'PPO' in model:\n",
    "        colors.append('#2ecc71')  # Green for PPO\n",
    "    elif 'A2C' in model:\n",
    "        colors.append('#27ae60')  # Darker green for A2C\n",
    "    elif 'REINFORCE' in model:\n",
    "        colors.append('#1abc9c')  # Teal for REINFORCE\n",
    "    elif 'DQN' in model:\n",
    "        colors.append('#3498db')  # Blue for DQN variants\n",
    "    else:\n",
    "        colors.append('#9b59b6')  # Purple for others\n",
    "\n",
    "bars = ax1.barh(df_sorted['Model'], df_sorted['Avg Score'], \n",
    "                xerr=df_sorted['Std Score'], capsize=3, color=colors, alpha=0.8)\n",
    "ax1.set_xlabel('Average Score (Food Eaten)', fontsize=12)\n",
    "ax1.set_title('Single-Snake Model Performance Comparison', fontsize=14)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score, std in zip(bars, df_sorted['Avg Score'], df_sorted['Std Score']):\n",
    "    ax1.text(bar.get_width() + std + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "             f'{score:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig1.savefig(FIGURES_DIR / 'single_snake_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {FIGURES_DIR / 'single_snake_comparison.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Algorithm Family Comparison\n",
    "\n",
    "# Group by algorithm family\n",
    "dqn_models = df[df['Model'].str.contains('DQN', case=False) & ~df['Model'].str.contains('Random|Shortest')]\n",
    "pg_models = df[df['Model'].str.contains('PPO|A2C|REINFORCE', case=False)]\n",
    "baselines = df[df['Model'].str.contains('Random|Shortest')]\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "families = ['Baselines', 'DQN Family', 'Policy Gradient']\n",
    "means = [\n",
    "    baselines['Avg Score'].mean() if len(baselines) > 0 else 0,\n",
    "    dqn_models['Avg Score'].mean() if len(dqn_models) > 0 else 0,\n",
    "    pg_models['Avg Score'].mean() if len(pg_models) > 0 else 0\n",
    "]\n",
    "stds = [\n",
    "    baselines['Avg Score'].std() if len(baselines) > 0 else 0,\n",
    "    dqn_models['Avg Score'].std() if len(dqn_models) > 0 else 0,\n",
    "    pg_models['Avg Score'].std() if len(pg_models) > 0 else 0\n",
    "]\n",
    "\n",
    "bars = ax2.bar(families, means, yerr=stds, capsize=5,\n",
    "               color=['#888888', '#3498db', '#2ecc71'], alpha=0.8)\n",
    "ax2.set_ylabel('Average Score', fontsize=12)\n",
    "ax2.set_title('Algorithm Family Comparison', fontsize=14)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{mean:.1f}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig2.savefig(FIGURES_DIR / 'algorithm_family_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {FIGURES_DIR / 'algorithm_family_comparison.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Flood-Fill Impact Analysis\n",
    "\n",
    "# Compare models with and without flood-fill\n",
    "flood_fill_comparison = []\n",
    "\n",
    "# Find matching pairs\n",
    "pairs = [\n",
    "    ('DQN MLP Basic', 'DQN MLP Flood-fill'),\n",
    "    ('Double DQN MLP', 'Double DQN MLP Flood-fill'),\n",
    "    ('Dueling DQN MLP', 'Dueling DQN MLP Flood-fill'),\n",
    "    ('PER DQN MLP', 'PER DQN MLP Flood-fill'),\n",
    "    ('PPO MLP', 'PPO MLP Flood-fill'),\n",
    "    ('REINFORCE MLP', 'REINFORCE MLP Flood-fill'),\n",
    "]\n",
    "\n",
    "for basic, floodfill in pairs:\n",
    "    if basic in results and floodfill in results:\n",
    "        flood_fill_comparison.append({\n",
    "            'Algorithm': basic.replace(' MLP Basic', '').replace(' MLP', ''),\n",
    "            'Without Flood-fill': results[basic]['avg_score'],\n",
    "            'With Flood-fill': results[floodfill]['avg_score'],\n",
    "            'Improvement': results[floodfill]['avg_score'] - results[basic]['avg_score']\n",
    "        })\n",
    "\n",
    "if flood_fill_comparison:\n",
    "    df_ff = pd.DataFrame(flood_fill_comparison)\n",
    "    \n",
    "    fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(df_ff))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax3.bar(x - width/2, df_ff['Without Flood-fill'], width, label='Without Flood-fill', color='#e74c3c', alpha=0.8)\n",
    "    bars2 = ax3.bar(x + width/2, df_ff['With Flood-fill'], width, label='With Flood-fill', color='#2ecc71', alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('Algorithm', fontsize=12)\n",
    "    ax3.set_ylabel('Average Score', fontsize=12)\n",
    "    ax3.set_title('Impact of Flood-Fill Features', fontsize=14)\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(df_ff['Algorithm'], rotation=45, ha='right')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig3.savefig(FIGURES_DIR / 'flood_fill_impact.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved: {FIGURES_DIR / 'flood_fill_impact.png'}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFlood-Fill Impact:\")\n",
    "    print(df_ff.to_string(index=False))\n",
    "else:\n",
    "    print(\"No matching pairs found for flood-fill comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Save Results to Files\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save full results as JSON\n",
    "json_results = {}\n",
    "for model_name, result in results.items():\n",
    "    # Remove scores array for cleaner JSON\n",
    "    clean_result = {k: v for k, v in result.items() if k != 'scores'}\n",
    "    json_results[model_name] = clean_result\n",
    "\n",
    "json_path = DATA_DIR / f'evaluation_results_{timestamp}.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "print(f\"Saved: {json_path}\")\n",
    "\n",
    "# Save summary CSV\n",
    "csv_path = DATA_DIR / f'single_snake_summary_{timestamp}.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved: {csv_path}\")\n",
    "\n",
    "# Also save a latest version for easy access\n",
    "latest_json = DATA_DIR / 'evaluation_results_latest.json'\n",
    "with open(latest_json, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "print(f\"Saved: {latest_json}\")\n",
    "\n",
    "latest_csv = DATA_DIR / 'single_snake_summary_latest.csv'\n",
    "df.to_csv(latest_csv, index=False)\n",
    "print(f\"Saved: {latest_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Final Summary\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nModels Evaluated: {len(results)}\")\n",
    "print(f\"Episodes per Model: {NUM_EPISODES}\")\n",
    "print(f\"Grid Size: {GRID_SIZE}x{GRID_SIZE}\")\n",
    "\n",
    "# Top 5 performers\n",
    "print(\"\\nTop 5 Performers:\")\n",
    "top5 = df.nlargest(5, 'Avg Score')\n",
    "for i, (_, row) in enumerate(top5.iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Model']}: {row['Avg Score']:.2f} +/- {row['Std Score']:.2f}\")\n",
    "\n",
    "# Best in each category\n",
    "print(\"\\nBest in Category:\")\n",
    "dqn_best = df[df['Model'].str.contains('DQN', case=False) & ~df['Model'].str.contains('Random|Shortest')].nlargest(1, 'Avg Score')\n",
    "if len(dqn_best) > 0:\n",
    "    print(f\"  DQN Family: {dqn_best.iloc[0]['Model']} ({dqn_best.iloc[0]['Avg Score']:.2f})\")\n",
    "\n",
    "pg_best = df[df['Model'].str.contains('PPO|A2C|REINFORCE', case=False)].nlargest(1, 'Avg Score')\n",
    "if len(pg_best) > 0:\n",
    "    print(f\"  Policy Gradient: {pg_best.iloc[0]['Model']} ({pg_best.iloc[0]['Avg Score']:.2f})\")\n",
    "\n",
    "print(\"\\nOutput Files:\")\n",
    "print(f\"  - Figures: {FIGURES_DIR}\")\n",
    "print(f\"  - Data: {DATA_DIR}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}