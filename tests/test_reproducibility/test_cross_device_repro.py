"""
Test cross-device reproducibility - compare JSON reports across devices

These tests work with reproducibility report JSON files generated by
scripts/verify_reproducibility.py on different devices.

Usage:
1. Generate reports on each device:
   python scripts/verify_reproducibility.py --seed 67 --output results_nvidia.json
   python scripts/verify_reproducibility.py --seed 67 --output results_amd.json

2. Run comparison tests:
   pytest tests/test_reproducibility/test_cross_device_repro.py \
       --report1 results_nvidia.json --report2 results_amd.json
"""

import pytest
import json
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from tests.test_reproducibility.utils import TOLERANCE, check_within_tolerance


@pytest.fixture
def report1(request):
    """Load first report from command line argument"""
    path = request.config.getoption("--report1")
    if path is None:
        pytest.skip("No --report1 provided")
    with open(path) as f:
        return json.load(f)


@pytest.fixture
def report2(request):
    """Load second report from command line argument"""
    path = request.config.getoption("--report2")
    if path is None:
        pytest.skip("No --report2 provided")
    with open(path) as f:
        return json.load(f)


class TestCrossDeviceReproducibility:
    """
    Test reproducibility across different devices

    These tests compare reproducibility reports generated on different
    devices (e.g., NVIDIA vs AMD GPUs) and verify results match within
    acceptable tolerances.
    """

    def test_seeds_match(self, report1, report2):
        """Verify both reports used the same seed"""
        seed1 = report1["metadata"]["seed"]
        seed2 = report2["metadata"]["seed"]
        assert seed1 == seed2, f"Seeds differ: {seed1} vs {seed2}"

    def test_cpu_random_matches(self, report1, report2):
        """CPU random should be identical across devices"""
        hash1 = report1["random_generation"]["cpu_rand_hash"]
        hash2 = report2["random_generation"]["cpu_rand_hash"]
        assert hash1 == hash2, "CPU random hashes differ - this should never happen"

        sum1 = report1["random_generation"]["cpu_rand_sum"]
        sum2 = report2["random_generation"]["cpu_rand_sum"]
        assert abs(sum1 - sum2) < 1e-10, f"CPU random sums differ: {sum1} vs {sum2}"

    def test_cpu_randint_matches(self, report1, report2):
        """CPU randint should be identical across devices"""
        hash1 = report1["random_generation"]["cpu_randint_hash"]
        hash2 = report2["random_generation"]["cpu_randint_hash"]
        assert hash1 == hash2, "CPU randint hashes differ - this should never happen"

    def test_network_weights_initialized_consistently(self, report1, report2):
        """Network weights should be initialized consistently"""
        # Network initialization happens on CPU before moving to GPU
        # so weights should match exactly
        hashes1 = report1["network"]["weight_hashes"]
        hashes2 = report2["network"]["weight_hashes"]

        for key in hashes1:
            assert key in hashes2, f"Missing weight key: {key}"
            # Weights initialized on CPU should be identical
            # But if GPU affects initialization, use tolerance
            if hashes1[key] != hashes2[key]:
                sum1 = report1["network"]["weight_sums"][key]
                sum2 = report2["network"]["weight_sums"][key]
                tol = TOLERANCE["cross_device_weights"]
                assert abs(sum1 - sum2) < tol["atol"], \
                    f"Weight sums differ beyond tolerance for {key}: {sum1} vs {sum2}"

    def test_gpu_random_comparison(self, report1, report2):
        """
        GPU random may differ between vendors - document the difference

        This test is informational. Different GPU vendors may use different
        random number generator implementations, so GPU random sequences
        are not expected to match exactly.
        """
        if "gpu_rand_hash" not in report1["random_generation"]:
            pytest.skip("No GPU random data in report1")
        if "gpu_rand_hash" not in report2["random_generation"]:
            pytest.skip("No GPU random data in report2")

        hash1 = report1["random_generation"]["gpu_rand_hash"]
        hash2 = report2["random_generation"]["gpu_rand_hash"]

        # Document whether they match (informational)
        if hash1 == hash2:
            print("GPU random hashes MATCH (same GPU vendor/implementation)")
        else:
            print("GPU random hashes DIFFER (expected for different GPU vendors)")
            # This is expected - different GPUs may have different RNG

    def test_environment_initial_state(self, report1, report2):
        """Environment initial state should match or be within tolerance"""
        obs_sum1 = report1["environment"]["initial_obs_sum"]
        obs_sum2 = report2["environment"]["initial_obs_sum"]

        tol = TOLERANCE["cross_device_observation"]
        diff = abs(obs_sum1 - obs_sum2)

        if diff < tol["atol"]:
            print(f"Initial observations match (diff={diff})")
        else:
            # If they differ, it's likely due to GPU random for food placement
            print(f"Initial observations differ by {diff} (GPU RNG difference)")

    def test_trajectory_rewards_comparison(self, report1, report2):
        """Compare trajectory rewards across devices"""
        rewards1 = report1["trajectory"]["rewards"]
        rewards2 = report2["trajectory"]["rewards"]

        assert len(rewards1) == len(rewards2), \
            f"Trajectory lengths differ: {len(rewards1)} vs {len(rewards2)}"

        total1 = report1["trajectory"]["total_reward"]
        total2 = report2["trajectory"]["total_reward"]

        # Document the difference
        diff = abs(total1 - total2)
        print(f"Total reward difference: {diff}")
        print(f"  Device 1: {total1}")
        print(f"  Device 2: {total2}")

        # With different GPU RNG, trajectories will diverge
        # so we just verify they're in a reasonable range
        tol = TOLERANCE["cross_device_loss"]  # Use relaxed tolerance
        rel_diff = abs(total1 - total2) / max(abs(total1), abs(total2), 1)

        if rel_diff > tol["rtol"]:
            print(f"WARNING: Large reward difference ({rel_diff*100:.1f}%) "
                  "- expected if GPU RNG differs")

    def test_forward_pass_output(self, report1, report2):
        """Forward pass outputs should match if inputs match"""
        # Check if inputs matched
        input_hash1 = report1["network"]["forward_input_hash"]
        input_hash2 = report2["network"]["forward_input_hash"]

        if input_hash1 == input_hash2:
            # Same input - outputs should match within tolerance
            out_sum1 = report1["network"]["forward_output_sum"]
            out_sum2 = report2["network"]["forward_output_sum"]

            tol = TOLERANCE["cross_device_q_values"]
            diff = abs(out_sum1 - out_sum2)
            assert diff < tol["atol"], \
                f"Forward pass outputs differ: {out_sum1} vs {out_sum2}"
        else:
            # Different inputs (due to GPU RNG) - skip comparison
            print("Forward pass inputs differ (GPU RNG), skipping output comparison")

    def test_loss_computation(self, report1, report2):
        """Loss values should match if weights and inputs match"""
        loss1 = report1["network"]["loss_value"]
        loss2 = report2["network"]["loss_value"]

        tol = TOLERANCE["cross_device_loss"]
        diff = abs(loss1 - loss2)

        print(f"Loss comparison: {loss1} vs {loss2} (diff={diff})")

        # Relaxed tolerance for loss due to accumulated differences
        if diff > tol["atol"]:
            rel_diff = diff / max(loss1, loss2, 1e-10)
            print(f"Loss differs by {rel_diff*100:.1f}%")

    def test_generate_comparison_summary(self, report1, report2):
        """Generate a summary of cross-device comparison"""
        summary = {
            "device1": report1["metadata"]["device"],
            "device2": report2["metadata"]["device"],
            "gpu1": report1["metadata"]["device_info"].get("gpu_name", "N/A"),
            "gpu2": report2["metadata"]["device_info"].get("gpu_name", "N/A"),
            "seed_match": report1["metadata"]["seed"] == report2["metadata"]["seed"],
            "cpu_random_match": (
                report1["random_generation"]["cpu_rand_hash"] ==
                report2["random_generation"]["cpu_rand_hash"]
            ),
        }

        # GPU random match (if available)
        if ("gpu_rand_hash" in report1["random_generation"] and
            "gpu_rand_hash" in report2["random_generation"]):
            summary["gpu_random_match"] = (
                report1["random_generation"]["gpu_rand_hash"] ==
                report2["random_generation"]["gpu_rand_hash"]
            )

        print("\n" + "=" * 50)
        print("CROSS-DEVICE COMPARISON SUMMARY")
        print("=" * 50)
        for key, value in summary.items():
            print(f"  {key}: {value}")
        print("=" * 50)


class TestReproducibilityReportGeneration:
    """Tests for generating reproducibility reports"""

    @pytest.fixture
    def seed(self):
        return 67

    def test_generate_report_cpu(self, seed, tmp_path):
        """Generate a reproducibility report on CPU"""
        from scripts.verify_reproducibility import run_verification

        results = run_verification(
            seed=seed,
            device_str="cpu",
            num_steps=10,
            num_envs=2
        )

        # Verify structure
        assert "metadata" in results
        assert "random_generation" in results
        assert "network" in results
        assert "environment" in results
        assert "trajectory" in results

        # Save and reload
        output_path = tmp_path / "test_report.json"
        with open(output_path, "w") as f:
            json.dump(results, f)

        with open(output_path) as f:
            loaded = json.load(f)

        assert loaded["metadata"]["seed"] == seed

    @pytest.mark.skipif(not __import__("torch").cuda.is_available(),
                        reason="CUDA not available")
    def test_generate_report_gpu(self, seed, tmp_path):
        """Generate a reproducibility report on GPU"""
        from scripts.verify_reproducibility import run_verification

        results = run_verification(
            seed=seed,
            device_str="cuda",
            num_steps=10,
            num_envs=2
        )

        assert "gpu_rand_hash" in results["random_generation"]
        assert results["metadata"]["device"] == "cuda"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
